\section*{601.465/665 --- Natural Language Processing \\ Reading for Homework 3: Smoothed Language Modeling}

\textbf{Prof. Jason Eisner --- Fall 2025}

We don’t have a required textbook for this course. Instead, handouts like this one are the main readings. This handout accompanies homework 3, which refers to it.

\subsection*{A Are $n$-gram models useful?}

Why build $n$-gram models when we know they are a poor linguistic theory? Answer: A linguistic system without statistics is often fragile, and may break when run on real data. It will also be unable to resolve ambiguities. So our first priority is to get some numbers into the system somehow. An $n$-gram model is a starting point, and may get reasonable results even though it doesn’t have any real linguistics yet.

\textbf{Speech recognition.} Speech recognition systems made heavy use of trigram models for decades. Alternative approaches that \textit{don’t} look at the trigrams do worse. One can do better by building fancy language models that combine trigrams with syntax, topic, and so on. But for a long time, you could only do a \textit{little} better—dramatic improvements over trigram models were hard to get. In the language modeling community, a rule of thumb was that you had enough for a Ph.D. dissertation if you had managed to reduce a good trigram model’s perplexity per word by 10\% (equivalent to reducing the cross-entropy by just 0.152 bits per word).

\textbf{Machine translation.} Statistical machine translation (MT) systems were originally developed in the late 1980’s and made use of trigram language models. After a quiet period, this paradigm was resurrected at the end of the century and started getting good practical results. Statistical MT systems often included 5-gram models trained on massive amounts of data.

Why 5-grams? Because an MT system that translates into English has to generate a new fluent sentence of English, and 5-grams do a better job than 3-grams of memorizing common phrases and local grammatical phenomena.

An English speech recognition system can get away without 5-grams because it is not generating a new English sentence. It observes the spoken version of an existing English sentence, and only has to guess what words the speaker \textit{actually} said. A 3-gram model helps to choose between “flower,” “flour,” and “floor” by using one word of context on either side. That already provides most of the value that we can get out of \textit{local} context. Going to a 5-gram model wouldn’t help too much with this choice, because it still wouldn’t look at enough of the sentence to determine whether we’re talking about gardening (“flower”), baking (“flour”), or cleaning (“floor”).

\subsection*{Smoothing}
Fancy smoothing techniques developed in the 1990’s, applied to trigram models, eventually managed to achieve up to a total 50\% reduction in perplexity per English word (equivalent to a cross-entropy reduction of 1 bit per word). A thorough review supported by careful comparative experiments can be found in \cite{Goodman2001}.

As they noted, however, improving perplexity didn’t reliably improve the error rate of the speech recognizer. In some sense, the speech recognizer only needs the language model to break ties among utterances that sound similar. Many improvements to perplexity didn’t happen to help break these ties.

\subsection*{Neural language models}
A line of work starting in 2000 used neural networks to produce smoothed probabilities for $n$-gram language models. These neural networks can be thought of as log-\textit{nonlinear} models—a generalization of the log-linear models considered in reading section F.4 below. The starting point for both is the word embeddings that were introduced on the previous homework.

Next, \textit{recurrent} neural networks (RNNs) became a popular way to get \textit{beyond} $n$-gram models. We will touch on these methods in this course. They are not limited to a fixed-length history. They can learn to exploit complex patterns in which the choice of next word is affected by the syntax, semantics, topic, style, and format of the left context.

RNN-based language models were originally proposed in 1991 by the inventor of RNNs, but there seem to be no published results on real data until 2010. In general, neural networks played little role in practical NLP until about 2014. Around then, thanks to a series of small innovations over the preceding decade in neural network architectures and parameter optimization, together with larger datasets and faster hardware, neural methods in NLP started to show real gains over traditional non-neural probabilistic methods. In particular, neural language models started to show real gains over $n$-gram models. However, it was noted that cleverly smoothed 7-gram models could still do about as well as an RNN model by looking at lots of features of the previous 6-gram \cite{Peleman2016}.

A new neural architecture called the Transformer, introduced in 2017, showed further empirical gains in language modeling, halving perplexity (i.e., saving 1 bit of cross-entropy) over RNN models of comparable size. Transformers quickly took over language modeling. We’ll look at them later.

\subsection*{More training data}
Of course, training on larger corpora helps any method! Wikipedia maintains a \href{https://en.wikipedia.org/wiki/List_of_large_language_models}{list of large language models}—mostly Transformer-based—including the sizes of their training corpora. A breakthrough model was GPT-3 \cite{Brown2020}, an enormous Transformer with 175 billion parameters, trained on 500 billion tokens\footnote{These tokens are actually word fragments, rather than words: see reading section D.6.} of (mostly) English text obtained by crawling the web. It achieved a perplexity of 20.5 on the Penn Treebank test set and is quite remarkably good at generating and extending text passages across a wide range of topics, styles, and formatting. Its creators showed that these abilities can be used to help solve many other NLP tasks, because the language model has to know a lot about language, meaning, and the real world in order to do such a good job of predicting what people are going to say next. GPT-3 formed the basis of ChatGPT.

\section*{B Boundary symbols}
Remember from the previous homework that a \textit{language model} estimates the \textit{probability of any word sequence} $\vec{w}$. In a trigram model, we use the chain rule and backoff to assume that
\[
p(\vec{w}) = \prod_{i=1}^{N} p(w_i \mid w_{i-2}, w_{i-1})
\]
with start and end boundary symbols handled as in the previous homework.

In other words, $w_N = \text{EOS}$ (“end of sequence”), while for $i < 1$, $w_i = \text{BOS}$ (“beginning of sequence”). Thus, $\vec{w}$ consists of $N-1$ words plus an EOS symbol. Notice that we do not generate BOS but we do condition on it (it was always there). Conversely, we do generate EOS but never condition on it (nothing follows it). The boundary symbols \texttt{BOS}, \texttt{EOS} are special symbols that do not appear among $w_1 \ldots w_{N-1}$.

In the homework that accompanies this reading, we will consider every \textit{line} in a file to implicitly be preceded by \texttt{BOS} and followed by \texttt{EOS}. A file might be a sentence, or an email message, or a text fragment.

\section*{C Datasets for Homework 3}
Homework 3 will mention corpora for three tasks: spam detection, language identification, and speech recognition. They are all at \url{http://cs.jhu.edu/~jason/465/hw-lm/data/}. Each corpus has a \texttt{README} file that you should look at.

\subsection*{C.1 The train/dev/test split}
Each corpus has already been divided for you into training, development, and test sets, which are in separate directories. You will use collect counts on \texttt{train}, tune the “hyperparameters” like $\lambda$ to maximize performance on \texttt{dev}, and then evaluate your performance on \texttt{test}.

In principle, you shouldn’t look at \texttt{test} until you’re ready to get the final results for a system, and then you must commit to reporting those results to avoid selective reporting. The danger of experimenting on \texttt{test} to improve performance on \texttt{test} is that you might “overfit” to it—that is, you might find your way to a method that seems really good, but is actually only good for that particular test set, not in general.

To be on the safe side, we will actually evaluate your system in a blind test, when we run it on new data you’ve never seen. Your grade will therefore be determined based on a \texttt{grading-test} set that you don’t have access to. So overfitting to \texttt{test} might give you good results on \texttt{test} in your writeup, but it will hurt you on \texttt{grading-test}. (Just as if you tell your boss that the system works great and is ready to ship, and then it doesn’t work for real users.)

\subsection*{C.2 Class ratios}
In the homework, you’ll have to specify a prior probability that a file will be genuine email (rather than spam) or English (rather than Spanish). In other words, how often do you expect the real world to produce genuine email or English in your test data? We will ask you to guess 0.7.

Of course, your system won’t know the true fraction on test data, because it doesn’t know the true classes—it is trying to predict them.

We can try to estimate the fraction from training data, or perhaps more appropriately from dev data (which are supposed to be “like the test data”). It happens that in dev data, $\tfrac{2}{3}$ of the documents are genuine email, and $\tfrac{1}{3}$ are English. In this case, the prior probability is a parameter or hyperparameter of the model, to be estimated from training or dev data as usual.

But if you think that test data might have a different rate of spam or Spanish than training data, then the prior probability is not necessarily something that you should represent within the model and estimate from training data. Instead it can be used to represent your personal guess about what you think test data \textit{will} be like.

Indeed, in the homework, you’ll use training data only to get the smoothed language models, which define the likelihood of the different classes. This leaves you free to specify your prior probability of the classes on the command line. This setup would let you apply the system to different test datasets about which you have different prior beliefs—the spam-infested email account that you abandoned, versus your new private email account that only your family knows about.

Does it seem strange to you that a guess or assumption might have a role in statistics? That is actually central to the Bayesian view of statistics—which says that you can’t get something for nothing. Just as you can’t get theorems without assuming axioms, you can’t get posterior probabilities without assuming prior probabilities.

\section*{D The vocabulary}
\subsection*{D.1 Choosing a finite vocabulary}
All the smoothing methods assume a finite vocabulary, so that they can allocate probability to all the words. But is this assumption justified? Aren’t there always new possible words of English that might show up in a test corpus (like \texttt{xyzzy} and \texttt{JacrobinsteinIndustries} and \texttt{fruitylicious})?

Yes there are \ldots so we will \textit{force} the vocabulary to be finite by a standard trick. Choose some fixed, finite vocabulary at the start. Then add one special symbol \texttt{OOV} that represents all other words. You should regard these other words as nothing more than variant spellings of the \texttt{OOV} symbol.

Note that \texttt{OOV} stands for “out of vocabulary,” not for “out of corpus,” so OOV words may have token count $> 0$ and in-vocabulary words may have count $= 0$.

\subsection*{D.2 Consequences for evaluating a model}
For example, when you are considering the test sentence
\begin{quote}
\texttt{i saw snuffleupagus on the tv}
\end{quote}
what you will actually compute is the probability of
\begin{quote}
\texttt{i saw OOV on the tv}
\end{quote}
which is really the \textit{total} probability of all sentences of the form
\begin{quote}
\texttt{i saw [some out-of-vocabulary word] on the tv}
\end{quote}

Admittedly, this total probability is higher than the probability of the \textit{particular} sentence involving \texttt{snuffleupagus}. But in most of this homework, we only wish to compare the probability of the snuffleupagus sentence under different models. Replacing \texttt{snuffleupagus} with \texttt{OOV} raises the sentence’s probability under all the models at once, so the \textit{comparison} is fair.\footnote{Homework question 10 and reading section D.6 discuss alternative approaches, which may also work better for text categorization, since they look at the spellings of the unfamiliar words rather than treating them all as identical OOVs.}

\subsection*{D.3 Comparing apples to apples}

We do have to make sure that if \texttt{snuffleupagus} is regarded as OOV by one model, then it is regarded as OOV by all the other models, too. It’s not appropriate to compare $p_{\text{model1}}(\text{i saw OOV on the tv})$ with $p_{\text{model2}}(\text{i saw snuffleupagus on the tv})$, since the former is actually the total probability of many sentences, and so will tend to be larger.

So all the models must have the \textit{same} finite vocabulary, chosen up front. In principle, this shared vocabulary could be \textit{any} list of words that you pick by \textit{any} means, perhaps using some external dictionary.

Even if the context “OOV on” never appeared in the training corpus, the smoothing method is required to give a reasonable value anyway to $p(\text{the} \mid \text{OOV, on})$, for example by backing off to $p(\text{the} \mid \text{on})$.

Similarly, the smoothing method must give a reasonable (non-zero) probability to $p(\text{OOV} \mid \text{i, saw})$. Because we’re merging all out-of-vocabulary words into a single word OOV, we avoid having to decide how to split this probability among them.

\subsection*{D.4 How to choose the vocabulary}

How should you choose the vocabulary? For this homework, simply take it to be the set of word types that appeared $\geq 3$ times anywhere in \textit{training} data. Then augment this set with a special OOV symbol. Let $V$ be the size of the resulting set (including OOV). Whenever you read a training or test word, you should immediately convert it to OOV if it’s not in the vocabulary. This is fast to check if you store the vocabulary in a hash set or an integerizer.

To help you understand/debug your programs, we have grafted brackets onto all out-of-vocabulary words in \textit{one} of the datasets (the \texttt{speech} directory, where the training data is assumed to be \texttt{train/switchboard}). This lets you identify such words at a glance. In this dataset, for example, we convert \texttt{uncertain} to \texttt{[uncertain]}—this doesn’t change its count, but does indicate that this is one of the words that your code will convert to OOV (if your code is correct).

\subsection*{D.5 Open-vocabulary language modeling}

The homework assumes a fixed finite vocabulary. However, an \textit{open-vocabulary} language model does not limit in advance to a finite vocabulary. Homework question 10 (extra credit) explores this possibility.

An open-vocabulary model must be able to assign positive probability to any word—that is, to any string of letters that might ever arise. If the \textit{alphabet} is finite, you could do this with a character $n$-gram model!

Such a model is sensitive to the spelling and length of the unknown word. Longer words will generally receive lower probabilities, which is why it is possible for the probabilities of all unknown words to sum to 1, even though there are infinitely many of them. (Just as $\tfrac{1}{2} + \tfrac{1}{4} + \tfrac{1}{8} + \tfrac{1}{16} + \cdots = 1.$)

\subsection*{D.6 Alternative tokenizations}

A language model is a probability distribution over sequences of tokens. But what are the tokens? We are working with text that has been tokenized into \textit{words}. That’s why our finite vocabulary is large and why OOV tokens are nonetheless rather common in test data.

An alternative would be to tokenize into \textit{characters}, as we did in the English/Spanish data. $V$ is now very small, but we still expect excellent coverage. For instance, with just English letters, digits, whitespace characters, and punctuation marks, we can keep $V < 100$ and still handle almost all English text. Any additional characters such as emojis, math symbols, and characters from non-English alphabets can be treated as OOV.

Of course, we are now in the business of predicting individual characters. And now a trigram language model only looks at the two previous characters, which is not very much context than the two previous words. So in this case, we would want $n$ in our $n$-gram model to be much larger than 3. (Or better yet, we’d use a neural language model.)

An intermediate option is to tokenize into \textit{subwords}. For example, we could use a morphological tokenizer that breaks the word \texttt{flopping} into two morphemes: the stem \texttt{flop} and the suffix \texttt{-ing}. The language model then predicts these tokens one at a time. It might assign positive probability to the sequence \texttt{flop -ed} in test data even if the word \texttt{flopped} had never occurred in training data.

But building a morphological tokenizer is a challenging language-specific problem—and not all unknown words can be decomposed into known morphemes (consider person names, drug names, or misspelled words).

Thus, language models in recent years have often \textit{learned} a subword tokenizer. Uncommon words are automatically split up into common word fragments—i.e., substrings that are common in the training corpus. (See footnote 1.) Thus, the training and test data are preprocessed into sequences of “word pieces” rather than words. In this case, \texttt{flopping} might be tokenized into \texttt{flop -ping} (or perhaps \texttt{flopp -ing}), which approximates the morphological analysis above. If all single-character strings are in the subword vocabulary, then OOV becomes unnecessary: it is possible to represent any string as a sequence of subwords.

\section*{E Evaluation metrics (also called ``evaluation loss functions'')}

In this homework, you will measure your performance in two ways—which we discussed early on in class. To measure the intrinsic predictive power of the model, you will use cross-entropy (per token). To measure how well the model does at the extrinsic task of text categorization, you will use error rate (per document). In both cases, \textit{smaller is better}.

Error rate may be what you really care about! However, it doesn’t give a lot of information on a small dev set. If your dev set has only 100 documents, then the error rate can only be one of the numbers $\{\tfrac{0}{100}, \tfrac{1}{100}, \cdots, \tfrac{100}{100}\}$. It can tell you if your changes helped by correcting a wrong answer. But it can’t tell you that your changes were “moving in the right direction” by merely increasing the probability of right answers.

In particular, for some of the tasks we are considering here, the error rate is just not very sensitive to the smoothing hyperparameter $\lambda$: there are many $\lambda$ values that will give the same integer number of errors on dev data. That is why you will use cross-entropy to select $\lambda$ on dev data: it will give you clearer guidance.

\subsection*{E.1 Other possible metrics}

As an alternative, could you devise a continuously varying version of the error rate? Yes, because our system doesn’t merely compute a single output class for each document.\footnote{Unlike a decision tree classifier, or a perceptron classifier that chooses the class with the highest score. Technically, you could regard the log-loss as a \textit{conditional cross-entropy} \ldots to be precise, it’s the conditional cross-entropy between empirical and system distributions over the output class. By contrast, the cross-entropy metric you’ll use on this homework is the cross-entropy between empirical and system distributions over the \textit{input} text. The output and the input are different random variables, so log-loss is quite different from the cross-entropy we’ve been using to evaluate a language model!} It constructs a probability distribution over those classes, using Bayes’ Theorem. So we can evaluate whether that distribution puts high probability on the correct answer.

\begin{itemize}
    \item One option is the \textit{expected error rate}. Suppose document \#1 is \texttt{gen}. If the system thinks $p(\text{gen} \mid \text{document1}) = 0.49$, then sadly the system will output \texttt{spam}, which ordinary error rate would count as 1 error. But suppose you pretend—just for evaluation purposes—that the system chooses its output randomly from its posterior distribution (“stochastic decoding” rather than “MAP decoding”). In that case, it only has probability 0.51 of choosing \texttt{spam}, so the \textit{expected} number of errors on this document is only 0.51. Partial credit!
    
    \item Another continuous error metric is the \textit{log-loss}, which is the system’s expected surprisal about the correct answer on the extrinsic task. The system’s surprisal on document 1 is $- \log_{2} p(\text{gen} \mid \text{document1}) = -\log_{2} 0.49 = 1.03$ bits.
\end{itemize}

Both expected error rate and log-loss are averages over the documents that are used to evaluate. So document 1 contributes 0.51 errors to the former average, and contributes 1.03 bits to the latter average.

In general, a single document contributes a number in $[0,1]$ to the expected error rate, but a number in $[0,\infty]$ to the log-loss. In particular, a system that thinks that $p(\text{gen} \mid \text{document1}) = 0$ is infinitely surprised by the correct answer (namely $-\log_{2}0 = \infty$). So optimizing for log-loss would dissuade you infinitely strongly from using this system \ldots basically on the grounds that a system that is completely confident in even one wrong answer can’t possibly have the correct probability distribution. To put it more precisely, if the dev set has size 100, then changing the system’s behavior on a single document can change the error rate or the expected error rate by at most $\tfrac{1}{100}$—after all, it’s just one document!—whereas it can change the log-loss by an \textit{unbounded} amount.

What is the relation between the log-loss and cross-entropy metrics? They are both average surprisals. However, they are very different:

\begin{center}
\begin{tabular}{|l|l|l|l|}
\hline
metric & what it evaluates & probability used & units & long docs count more? \\
\hline
log-loss & the whole classification system $p(\text{gen} \mid \text{document1})$ & bits per document & no \\
cross-entropy & the \texttt{gen} model within the system $p(\text{document1} \mid \text{gen})$ & bits per gen token & yes \\
\hline
\end{tabular}
\end{center}

\subsection*{E.2 Generative vs. discriminative}

There is an important difference in style between these metrics.
Our cross-entropy (or perplexity) is a \textit{generative metric} because it measures how likely the system would 
to randomly generate the observed test data. In other words, it evaluates how well the system predicts the 
test data.\footnote{In fact, a fully generative metric would require the system to \textit{fully} predict the test data—not only the documents but also their classes. That metric would be the joint log-likelihood, namely, $\log_2 \prod_i p(document_i, class_i) = \sum_i \log_2 p(document_i \mid class_i) \cdot p(class_i)$. The second factor here is the prior probability of the class (e.g., gen or spam), which would also have to be specified as part of the model.}  

The error rate, expected error rate, and log-loss are all said to be \textit{discriminative metrics} because they 
only measure how well the system discriminates between correct and incorrect classes. This is more focused 
on the particular task, which is good; but it considers less information from the test data. In other words, the 
metric has less bias, in the sense that it is measuring what we actually care about, but it has higher variance 
from test set to test set, and thus is less reliable on a small test set.  

In short, a discriminative setup focuses less on explaining the input data and more on solving a particular 
task—less science, more engineering. The generative vs. discriminative terminology is widely used across 
NLP and ML:  

\textbf{evaluation (test data)} We compared generative vs. discriminative evaluation methods above.  

\textbf{tuning (dev data)} Methods for setting hyperparameters may optimize either a generative or discriminative 
metric on the development data. (Normally they would use the evaluation metric, to match the actual 
evaluation condition.)  

\textbf{training (train data)} Similarly, methods for setting parameters may optimize either a generative or dis-
criminative metric on the development data. These are called generative or discriminative training 
methods, respectively.  

It is possible to use generative training as we are doing on this assignment (so that training gets to 
consider more information from the training data) but still use discriminative methods for tuning and 
evaluation (because ultimately we care about the engineering task).  

\textbf{modeling} A \textit{generative model} includes a probability distribution $p(\text{input})$ that accounts for the input data. 
Thus, this homework uses generative models (namely language models).  

A discriminative model only tries to predict output from input, possibly using $p(\text{output}\mid\text{input})$. For 
example, a conditional log-linear model for text classification would be discriminative. This kind of 
model does not even define $p(\text{input})$, so it can’t be used for generative training or evaluation.  

\section*{F \quad Smoothing techniques}

Here are the smoothing techniques we’ll consider, writing $\hat{p}$ for our \textit{smoothed estimate} of $p$.  

\subsection*{F.1 \quad Uniform distribution}

$\hat{p}(z \mid xy)$ is the same for every $xyz$; namely,  
\begin{equation}
\hat{p}(z \mid xy) = \frac{1}{V}
\end{equation}  
where $V$ is the size of the vocabulary \textit{including OOV}.  

\subsection*{F.2 \quad Add-$\lambda$}

Add a constant $\lambda \geq 0$ to every trigram count $c(xyz)$:  
\begin{equation}
\hat{p}(z \mid xy) = \frac{c(xyz)+\lambda}{c(xy)+\lambda V}
\end{equation}  
where $V$ is defined as above. (Observe that $\lambda=1$ gives the add-one estimate. And $\lambda=0$ gives the naive 
historical estimate $c(xyz)/c(xy)$.)  

\subsection*{F.3 \quad Add-$\lambda$ with backoff}

Suppose both $z$ and $z'$ have rarely been seen in context $xy$. These small trigram counts are unreliable, so 
we’d like to rely largely on backed-off bigram estimates to distinguish $z$ from $z'$:  

\begin{equation}
\hat{p}(z \mid xy) = \frac{c(xyz)+\lambda V \cdot \hat{p}(z \mid y)}{c(xy)+\lambda V}
\end{equation}  

where $\hat{p}(z \mid y)$ is a backed-off bigram estimate, which is estimated recursively by a similar formula. (If 
$\hat{p}(z \mid y)$ were the uniform estimate $1/V$ instead, this scheme would be identical to add-$\lambda$.)  

So the formula for $\hat{p}(z \mid xy)$ backs off to $\hat{p}(z \mid y)$, whose formula backs off to $\hat{p}(z)$, whose formula 
backs off to \dots what?? Figure it out!  

\subsection*{F.4 \quad Conditional log-linear modeling}

In the previous homework, you learned how to construct log-linear models. Here’s that tutorial reading 
again, but let’s restate the construction in our current notation.\footnote{Unfortunately, the log-linear tutorial reading also used the variable names $x$ and $y$, but to mean something different than they mean in this homework. Its notation is pretty standard in machine learning.}  

Given a trigram $xyz$, our model $p$ is defined by  
\begin{equation}
p(z \mid xy) \overset{\text{def}}{=} \frac{\tilde{p}(xyz)}{Z(xy)}
\end{equation}  

where  
\begin{equation}
\tilde{p}(xyz) \overset{\text{def}}{=} \exp\left(\sum_k \theta_k \cdot f_k(xyz)\right) = \exp\left(\vec{\theta} \cdot \vec{f}(xyz)\right)
\end{equation}  

\begin{equation}
Z(xy) \overset{\text{def}}{=} \sum_z \tilde{p}(xyz)
\end{equation}  

Here $\vec{f}(xyz)$ is the feature vector extracted from $xyz$, and $\vec{\theta}$ is the model’s weight vector. $\sum_z$ sums over the 
$V$ words in the vocabulary (including OOV) in order to ensure that you end up with a probability distribution 
over this chosen vocabulary.  

The resulting distribution $p$ depends on the value of $\vec{\theta}$. Training on data (see reading section H below) 
finds a particular estimate of $\vec{\theta}$ (which we could call $\hat{\vec{\theta}}$), which yields our smoothed distribution $\hat{p}$. Stronger 
regularization during training gives stronger smoothing.  

\subsection*{F.4.1 \quad Bigrams and skip-bigram features from word embeddings}

What features should we use in the log-linear model?  

A natural idea is to use one binary feature for each specific unigram $z$, bigram $yz$, and trigram $xyz$ (see 
reading section J.3 below).  

Instead, however, let’s start with the following model based on word embeddings:  
\begin{equation}
\tilde{p}(xyz) \overset{\text{def}}{=} \exp\left(\vec{x}^T X \vec{z} + \vec{y}^T Y \vec{z}\right)
\end{equation}  

where the vectors $\vec{x}, \vec{y}, \vec{z}$ are specific $d$-dimensional embeddings of the word types $x, y, z$, while $X, Y$ are 
$d \times d$ matrices. The $T$ superscript is the matrix transposition operator, used here to transpose a column 
vector to get a row vector.  

This model may be a little hard to understand at first, so here’s some guidance.  

\textbf{What’s the role of the word embeddings?} Note that the language model is still defined as a conditional 
probability distribution over the \textit{vocabulary}. The \textit{lexicon}, which you will specify on the command line, 
is merely an external resource that lets the model look up some attributes of the vocabulary words. Just 
like the dictionary on your shelf, it may also list information about some words you don’t need, and it 
may lack information about some words you do need. In short, the existence of a lexicon doesn’t affect 
the interpretation of $\sum_z$ in (6): that formula remains the same regardless of whether the model’s features 
happen to consult a lexicon!  

For OOV, or for any other type in your vocabulary that has no embedding listed in the lexicon, your 
features should back off to the embedding of OOL—a special “out of lexicon” symbol that stands for “all 
other words.” OOL is listed in the lexicon, just as OOV is included in the vocabulary.  

Note that even if an specific out-of-vocabulary word is listed in the lexicon, you must not use that listing.\footnote{This issue would not arise if we simply defined the vocabulary to be the set of words that appear in the lexicon. This simple strategy is certainly sensible, but it would slow down normalization because our lexicon is quite large.}  

For an out-of-vocabulary word, you are supposed to be computing probabilities like $p(OOV \mid xy)$, which 
is the probability of the whole OOV class—it doesn’t even mention the specific word that was replaced by 
OOV. (See reading section D.2.)  

\textbf{Is this really a log-linear model?} Now, what’s up with (7)? It’s a valid formula: you can always get 
a probability distribution by defining $\hat{p}(z \mid xy) = \frac{1}{Z(xy)} \exp(\text{any function of } x,y,z \text{ that you like})$! But 
is (7) really a log-linear function? Yes it is! Let’s write out those $d$-dimensional vector-matrix-vector 
multiplications more explicitly:  

\begin{equation}
\tilde{p}(xyz) = \exp\left(\sum_{j=1}^d \sum_{m=1}^d x_j X_{jm} z_m + \sum_{j=1}^d \sum_{m=1}^d y_j Y_{jm} z_m \right)
\end{equation}  

\begin{equation}
= \exp\left(\sum_{j=1}^d \sum_{m=1}^d X_{jm} \cdot (x_j z_m) + \sum_{j=1}^d \sum_{m=1}^d Y_{jm} \cdot (y_j z_m) \right)
\end{equation}  
This does have the log-linear form of (5). Suppose $d=2$. Then implicitly, we are using a weight vector $\vec{\theta}$ of length $d^2+d^2=8$, defined by
\[
\langle \theta_1, \theta_2, \theta_3, \theta_4, \theta_5, \theta_6, \theta_7, \theta_8 \rangle
\quad\quad
\downarrow \quad \downarrow \quad \downarrow \quad \downarrow \quad \downarrow \quad \downarrow \quad \downarrow \quad \downarrow
\]
\[
\langle X_{11}, X_{12}, X_{21}, X_{22}, Y_{11}, Y_{12}, Y_{21}, Y_{22} \rangle
\tag{10}
\]
for a vector $\vec{f}(xyz)$ of 8 features
\[
\langle f_1(xyz), f_2(xyz), f_3(xyz), f_4(xyz), f_5(xyz), f_6(xyz), f_7(xyz), f_8(xyz)\rangle
\quad\quad
\downarrow \quad \downarrow \quad \downarrow \quad \downarrow \quad \downarrow \quad \downarrow \quad \downarrow \quad \downarrow
\]
\[
\langle x_1z_1, \; x_1z_2, \; x_2z_1, \; x_2z_2, \; y_1z_1, \; y_1z_2, \; y_2z_1, \; y_2z_2 \rangle
\tag{11}
\]

Remember that the optimizer’s job is to automatically manipulate some control sliders. This particular model with $d=2$ has a control panel with 8 sliders, arranged in two $d \times d$ grids ($X$ and $Y$). The point is that we can also refer to those same 8 sliders as $\theta_1,\ldots,\theta_8$ if we like. What features are these sliders (weights) be connected to? The ones in (11): if we adopt those feature definitions, then our general log-linear formula (5) will yield up our specific model (9) ( = (7)) as a special case. 

What keeps (7) log-linear is that the feature functions are pre-specified functions of $xyz$ as shown in equation (11). Specifically, they are determined by the word embeddings $\vec{x}, \vec{y},$ and $\vec{z}$. We are not going to learn the word embeddings or the feature functions $f_k$ — we only have to learn the weights $\theta_k$.  

As always, the learned weight vector $\vec{\theta}$ is incredibly important: it determines all the probabilities in the model.  

\subsection*{Is this a sensible model?}
The feature definitions in (11) are pairwise products of embedding dimensions. Why on earth would such features be useful? First imagine that the embedding dimensions were bits (0 or 1). Then $x_2z_1=1$ iff ($x_2=1$ and $z_1=1$), so you could think of multiplication as a kind of \textit{feature conjunction}. Multiplication has a similar conjunctive effect even when the embedding dimensions are in $\mathbb{R}$. For example, suppose $z_1>0$ indicates the degree to which $z$ is a human-like noun, while $x_2>0$ indicates the degree to which $x$ is a verb whose direct objects are usually human. Then the product $x_2 z_1$ will be larger for trigrams like \textit{kiss the baby} and \textit{marry the cop}. So by learning a positive weight $X_{21}$ (nicknamed $\theta_3$), the optimizer can drive $\hat{p}(\text{baby}|\text{kiss the})$ higher, at the expense of probabilities like $\hat{p}(\text{benzene}|\text{kiss the})$. $\hat{p}(\text{bunny}|\text{kiss the})$ might be somewhere in the middle since bunnies are a bit human-like and thus $bunny_1$ might be numerically somewhere between $baby_1$ and $benzene_1$.

\subsection*{Example}
As an example, let’s calculate the letter trigram probability $\hat{p}(s|er)$. Suppose the relevant letter embeddings and the feature weights are given by
\[
\vec{e} = \begin{bmatrix}-0.5 \\ 1 \end{bmatrix}, \quad 
\vec{r} = \begin{bmatrix} 0 \\ 0.5 \end{bmatrix}, \quad
\vec{s} = \begin{bmatrix} 0.5 \\ 0.5 \end{bmatrix}, \quad
X = \begin{bmatrix} 1 & 0 \\ 0 & 0.5 \end{bmatrix}, \quad
Y = \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix}.
\]

First, we compute the unnormalized probability.
\[
\tilde{p}(ers) = \exp \left( [-0.5 \;\; 1] 
\begin{bmatrix} 1 & 0 \\ 0 & 0.5 \end{bmatrix} 
\begin{bmatrix} 0.5 \\ 0.5 \end{bmatrix}
+ [0 \;\; 0.5] 
\begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix} 
\begin{bmatrix} 0.5 \\ 0.5 \end{bmatrix} \right)
\]
\[
= \exp(-0.5 \times 1 \times 0.5 + 1 \times 0.5 \times 0.5 + 0 \times 2 \times 0.5 + 0.5 \times 1 \times 0.5) = \exp(0.25) = 1.284
\]

We then normalize $\tilde{p}(ers)$.
\[
\hat{p}(s|er) \;\stackrel{\text{def}}{=}\; \frac{\tilde{p}(ers)}{Z(er)} = \frac{\tilde{p}(ers)}{\tilde{p}(era) + \tilde{p}(erb) + \cdots + \tilde{p}(erz)} = \frac{1.284}{1.284 + \cdots}
\tag{12}
\]

\subsection*{Speedup}
The example illustrates that the denominator
\[
Z(xy) = \sum_{z'} \tilde{p}(xyz') = \sum_{z'} \exp\big(\vec{x}^T X \vec{z'} + \vec{y}^T Y \vec{z'}\big)
\tag{13}
\]
is expensive to compute because of the summation over all $z'$ in the vocabulary. Fortunately, you can compute $\vec{x}^T X z'$ for all $z'$ simultaneously. The results can be found as the elements of the row vector $\vec{x}^T X E$, where $E$ is a $d \times V$ matrix whose columns are the embeddings of the various words $z'$ in the vocabulary. This is easy to see, and computing this vector still requires just as many scalar multiplications and additions as before \ldots but we have now expressed the computation as a pair of vector-matrix multiplications, $(\vec{x}^T X)E$, which you can perform using library calls in PyTorch (similarly to another matrix library, numpy). That can be considerably faster than a Python loop over all $z'$. That is because the library call is highly optimized and exploits hardware support for matrix operations (e.g., parallelism).

\subsection*{F.5 Other smoothing schemes}
Numerous other smoothing schemes exist. In past years, for example, our course homeworks have used Witten-Bell backoff smoothing, or Katz backoff with Good–Turing discounting.  

In practical settings, the most popular $n$-gram smoothing scheme is something called modified Kneser–Ney. One can also use a more principled Bayesian method based on the hierarchical Pitman–Yor process; the resulting formulas are very close to modified Kneser–Ney.  

Remember: While these techniques are effective, a really good language model would do more than just smooth $n$-gram probabilities well. To predict a word sequence as accurately as a human can finish another human’s sentence, it would go beyond the whole $n$-gram family to consider syntax, semantics, and topic throughout a sentence or document. It would also use common sense and factual knowledge about the world. Thus, language modeling remains an active area of research that uses grammars, recurrent neural networks, and other techniques.  

Indeed, it is reasonable to say that language modeling is AI-complete. That is, we can’t solve language modeling without solving pretty much all of AI. This was essentially the point of Alan Turing’s 1950 article “Computing Machinery and Intelligence,” in which he considered the possibility of a machine that could sustain a deep, wide-ranging conversation. His “Turing Test” involves making you guess whether you’re conversing with a computer or a human. If you can’t tell, then maybe you should accept that the computer is intelligent in some sense.  

(It follows that progress in language modeling might inadvertently \textit{create} progress in AI. The enormous GPT-3 model (reading section A) is already startlingly good at generating sentences that are appropriate in context. It has no explicit representation of grammar, knowledge, or reasoning—yet by learning how to model its training corpus, it has implicitly picked up a lot of whatever is needed to generate intelligent text. While it is still imperfect in many ways, its creators demonstrated that we can interrogate it to get answers to other AI problems that it was not specifically designed to solve.)  

\section*{G Safe practices for working with log-probabilities}

\subsection*{G.1 Use natural log for internal computations}
In this homework, as in most of mathematics, log means $\log_e$ (the log to base $e$, or natural log, sometimes written ln). This is also the standard behavior of the \texttt{log} function in most programming languages.  

With natural log, the calculus comes out nicely, thanks to the fact that $\frac{d}{dz}\log Z = \frac{1}{Z}$. It’s only with natural log that the gradient of the log-likelihood of a log-linear model can be directly expressed as observed features minus expected features.  

On the other hand, information theory conventionally talks about bits, and quantities like entropy and cross-entropy are conventionally measured in bits. Bits are the unit of $-\log_2$ probability. A probability of 0.25 is reported “in negative-log space” as $-\log_2 0.25 = 2$ bits. Some people do report that value more simply as $-\log_e 0.25 = 1.386$ nats. But it is more conventional to use bits as the unit of measurement. (The term “bits” was coined in 1948 by Claude Shannon to refer to “binary digits,” and “nats” was later defined by analogy to refer to the use of natural log instead of log base 2. The unit conversion factor is $\frac{1}{\log 2} \approx 1.443$ bits/nat.)  

Even if you are planning to \textit{print} bit values, it’s still wise to standardize on $\log_e$-probabilities for all of your formulas, variables, and internal computations. Why? They’re just easier! If you tried to use negative $\log_2$-probabilities throughout your computation, then whenever you called the \texttt{log} function or took a derivative, you’d have to remember to convert the result. It’s too easy to make a mistake by omitting this step or by getting it wrong. So the best practice is to do this unit conversion \textit{only} when you print: at that point convert your $\log_e$-probability from negative nats to positive bits by dividing by $-\log 2 \approx -0.693$.

\subsection*{G.2 Avoid exponentiating big numbers (crucial for gen/spam!)}
Log-linear models require calling the $\exp$ function. Unfortunately, $\exp(710)$ is already too large for a 64-bit floating-point number to represent, and will generate a runtime error (“overflow”). Conversely, $\exp(-746)$ is too close to 0 to represent, and will simply return 0 (“underflow”).  

That shouldn’t be a problem for this homework if you stick to the language ID task. If you are experiencing an overflow issue there, then your parameters probably became too positive or too negative as you ran stochastic gradient descent, or because of the way you randomly initialized your model’s parameters.  

But to avoid these problems elsewhere—including with the spam detection task—the standard trick is to represent all values “in log-space.” In other words, simply store 710 and -746 rather than attempting to exponentiate them.

But how can you do arithmetic in log-space? Suppose you have two numbers $p, q$, which you are representing in memory by their logs, $lp$ and $lq$.  

\begin{itemize}
    \item \textbf{Multiplication:} You can represent $pq$ by its log, $\log(pq) = \log(p) + \log(q) = lp + lq$. That is, multiplication corresponds to log-space addition.

    \item \textbf{Division:} You can represent $p/q$ by its log, $\log(p/q) = \log(p) - \log(q) = lp - lq$. That is, division corresponds to log-space subtraction.

    \item \textbf{Addition:} You can represent $p+q$ by its log. $\log(p+q) = \log(\exp(lp)+\exp(lq)) = \texttt{logaddexp}(lp,lq)$. See the discussion of \texttt{logaddexp} and \texttt{logsumexp} in the future Homework 6 handout. These are available in PyTorch.
\end{itemize}

For training a log-linear model, you can work almost entirely in log space, representing $u$ and $Z$ in memory by their logs, $\log u$ and $\log Z$. In order to compute the expected feature vector in (18) below, you will need to come out of log space and find $p(z' \mid xy) = u'/Z$ for each word $z'$. But computing $u'$ and $Z$ separately is dangerous: they might be too large or too small. Instead, rewrite $p(z' \mid xy)$ as $\exp(\log u' - \log Z)$. Since $u' \leq Z$, this is $\exp$ of a negative number, so it will never \emph{overflow}. It might \emph{underflow} to 0 for some words $z'$, but that’s ok: it just means that $p(z' \mid xy)$ really is extremely close to 0, and so $\vec{f}(xyz')$ should make only a negligible contribution to the expected feature vector.

\section*{H Training a log-linear model}

\subsection*{H.1 The training objective}

To implement the conditional log-linear model, the main work is to train $\vec{\theta}$ (given some training data and a regularization coefficient $C$). As usual, you’ll set $\vec{\theta}$ to maximize
\begin{equation}
F(\vec{\theta}) \stackrel{\text{def}}{=} \frac{1}{N} \left( \sum_{i=1}^N \log \hat{p}(w_i \mid w_{i-2} w_{i-1}) \;-\; C \cdot \sum_k \theta_k^2 \right)
\end{equation}
which is the $L_2$-regularized log-likelihood per word token. (There are $N$ word tokens.)

So we want $\vec{\theta}$ to make our training corpus probable, or equivalently, to make the $N$ events in the corpus (including the final \texttt{EOS}) probable on average given their bigram contexts. At the same time, we also want the weights in $\vec{\theta}$ to be close to 0, other things equal (regularization).\footnote{As explained on the previous homework, this can also be interpreted as maximizing $p(\vec{\theta} \mid \vec{w})$---that is, choosing the most probable $\vec{\theta}$ given the training corpus. By Bayes’ Theorem, $p(\vec{\theta} \mid \vec{w})$ is proportional to 
\[
p(\vec{w} \mid \vec{\theta}) \cdot p(\vec{\theta})
\]
where the first term is likelihood and the second is prior.}  

The regularization coefficient $C \geq 0$ can be selected based on dev data.

\subsection*{H.2 Stochastic gradient descent}

Fortunately, concave functions like $F(\vec{\theta})$ in (14) are “easy” to maximize. You can implement a simple \textit{stochastic gradient descent (SGD)} method to do this optimization.  

More properly, this should be called \textit{stochastic gradient ascent}, since we are maximizing rather than minimizing, but that’s just a simple change of sign. The pseudocode is given by Algorithm 1. We rewrite the objective $F(\vec{\theta})$ given in (14) as an average of local objectives $F_i(\vec{\theta})$ that each predict a single word, by moving the regularization term into the summation.
\begin{align}
F(\vec{\theta}) &= \frac{1}{N} \sum_{i=1}^N \left( \log \hat{p}(w_i \mid w_{i-2} w_{i-1}) - \frac{C}{N}\cdot \sum_k \theta_k^2 \right) \\
&= \frac{1}{N} \sum_{i=1}^N F_i(\vec{\theta})
\end{align}

The gradient of this average, $\nabla F(\vec{\theta})$, is therefore the \emph{average} value of $\nabla F_i(\vec{\theta})$.

\begin{algorithm}[H]
\caption{Stochastic gradient ascent}
\begin{algorithmic}[1]
\Procedure{Train}{}
\State $\vec{\theta} \gets \vec{\theta}^{(0)}$
\State $t \gets 0$ \Comment{number of updates so far}
\For{$e \gets 1 \to E$} \Comment{do $E$ passes over the training data, or ``epochs''}
    \For{$i \gets 1 \to N$} \Comment{loop over summands of (16)}
        \State $\eta \gets \frac{\eta_0}{1 + \frac{\eta_0}{2C} \cdot t}$ \Comment{current stepsize—decreases gradually}
        \State $\vec{\theta} \gets \vec{\theta} + \eta \cdot \nabla F_i(\vec{\theta})$ \Comment{move $\vec{\theta}$ slightly in a direction that increases $F_i(\vec{\theta})$}
        \State $t \gets t+1$
    \EndFor
\EndFor
\State \Return $\vec{\theta}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\noindent \textbf{Discussion.} On each iteration, the algorithm picks some word $i$ and pushes $\vec{\theta}$ in the direction $\nabla F_i(\vec{\theta})$, which is the direction that gets the fastest increase in $F_i(\vec{\theta})$. The updates from different $i$ will partly cancel one another out,\footnote{For example, in the training sentence \textit{eat your dinner but first eat your words}, $\nabla F_3(\vec{\theta})$ is trying to raise the probability of \texttt{dinner}, while $\nabla F_8(\vec{\theta})$ is trying to raise the probability of \texttt{words} (at the expense of \texttt{dinner}!) in the same context.} but their \emph{average} direction is $\nabla F(\vec{\theta})$, so their average effect will be to improve the overall objective $F(\vec{\theta})$. Since we are training a log-linear model, our $F(\vec{\theta})$ is a concave function with a single global maximum; a theorem guarantees that the algorithm will converge to that maximum if allowed to run forever ($E = \infty$).  

How far the algorithm pushes $\vec{\theta}$ is controlled by $\eta$, known as the “step size” or “learning rate.” This starts at $\eta_0$, but needs to decrease over time in order to guarantee convergence of the algorithm. The rule in line 6 for gradually decreasing $\eta$ works well with our specific $L_2$-regularized objective (14).\footnote{It is based on the discussion in section 5.2 of Bottou (2012), ``Stochastic gradient descent tricks,'' who has this objective as equation (10). You should read that paper in full if you want to use SGD ``for real'' on your own problems.}

Note that $t$ increases and the stepsize decreases on every pass through the inner loop. This is important because $N$ might be extremely large in general. Suppose you are training on the whole web—then the stochastic gradient ascent algorithm should have essentially converged even before you finish the first epoch!  

\subsection*{H.3 The gradient vector}

The gradient vector $\nabla F_i(\vec{\theta})$ is merely the vector of partial derivatives 
\[
\left( \frac{\partial F_i(\vec{\theta})}{\partial \theta_1}, \frac{\partial F_i(\vec{\theta})}{\partial \theta_2}, \ldots \right).
\]
where $F_i(\vec{\theta})$ was defined in (16). As you’ll recall from the previous homework, each partial derivative takes a simple and beautiful form\footnote{This formula shows the partial derivative with respect to $\theta_k$ only. If you prefer to think of computing the whole gradient vector, for all $k$ at once, you can equivalently write this using vector computations as 
\[
\nabla F_i(\vec{\theta}) = \vec{f}(xyz) - \sum_{z'} \hat{p}(z' \mid xy)\vec{f}(xyz') - \frac{2C}{N}\vec{\theta}
\]}:
\begin{equation}
\frac{\partial F_i(\vec{\theta})}{\partial \theta_k} = f_k(xyz) \;-\; \sum_{z'} \hat{p}(z' \mid xy) f_k(xyz') \;-\; \frac{2C}{N}\theta_k
\end{equation}

\subsection*{H.4 The gradient for the embedding-based model}

If we use the embedding features (9), the features are defined in terms of the $X$ and $Y$ matrices, as shown in (9). The partial derivatives with respect to these weights are
\begin{align}
\frac{\partial F_i(\vec{\theta})}{\partial X_{jm}} &= x_j z_m - \sum_{z'} \hat{p}(z' \mid xy)x_j z'_m - \frac{2C}{N}X_{jm} \\
\frac{\partial F_i(\vec{\theta})}{\partial Y_{jm}} &= y_j z_m - \sum_{z'} \hat{p}(z' \mid xy)y_j z'_m - \frac{2C}{N}Y_{jm}
\end{align}
where as before, we use $\vec{x}, \vec{y}, \vec{z}, \vec{z'}$ to denote the embeddings of the words $x, y, z, z'$. Thus, the update to $\vec{\theta}$ (Algorithm 1 line 7) is
\begin{align}
(\forall j,m = 1,2,\ldots,d) \quad X_{jm} &\gets X_{jm} + \eta \cdot \frac{\partial F_i(\vec{\theta})}{\partial X_{jm}} \\
(\forall j,m = 1,2,\ldots,d) \quad Y_{jm} &\gets Y_{jm} + \eta \cdot \frac{\partial F_i(\vec{\theta})}{\partial Y_{jm}}
\end{align}

\section*{ I Practical hints for stochastic gradient ascent}

The magenta hyperlinks in this section may be particularly useful. Most of them link to PyTorch documentation.

\subsection*{I.1 Use automatic differentiation}

You don’t actually have to implement the gradient computations in reading sections H.3 to H.4! You could, of course. But the \texttt{backward} method in PyTorch will automatically compute the vector of partial derivatives for you, using a technique known as automatic differentiation by back-propagation (or simply ``back-prop"). So, you only have to implement the ``forward" computation $F_i(\vec{\theta})$ for a given example $i$, and PyTorch will be able to ``work backward" and find $\nabla F_i(\vec{\theta})$. This requires it to determine how small changes to $\vec{\theta}$ would have affected $F_i(\vec{\theta})$. See reading section I.6 below for details.

If the forward computation is efficient and correct, then the backward computation as performed by \texttt{backward} will also be efficient and correct. If instead you wrote your own code to compute the gradient, it would be easy to mess up and miss an algebraic optimization—you could end up taking $O(V^2)$ time when $O(V)$ is possible. You’d also have to put in checks to make sure that you were actually computing the gradient correctly (e.g., the ``finite-difference check"). This is not necessary nowadays.

\subsection*{I.2 Don’t try to learn all of PyTorch}

We won’t be learning the whole \href{https://pytorch.org}{PyTorch framework} in this class, although it’s very nicely designed. All we need is the basics:
\begin{itemize}
    \item rapidly manipulate \href{https://pytorch.org/docs/stable/tensors.html}{tensors} (multidimensional numeric arrays), just as in NumPy, Matlab, and other scientific computing packages
    \item automatically compute gradients via \texttt{backward}
\end{itemize}

(If you’ve built neural nets in PyTorch before, for example in the Deep Learning class, then you may have written models by extending \texttt{torch.nn.Module} and instantiating the \texttt{forward} method. However, \textbf{we’re going to mostly avoid using \texttt{torch.nn}}. If you’re used to relying on it, check out \href{https://pytorch.org/tutorials/beginner/nn_tutorial.html}{Neural net from scratch} to see how to train a network with the PyTorch basics only. That link is the starting point for an explanation of how \texttt{torch.nn} can optionally be used to streamline the code, but you don’t need that.)

\subsection*{I.3 Make the forward computation efficient}

In general, use PyTorch’s \href{https://pytorch.org/docs/stable/tensors.html\#torch.Tensor}{tensor operations} wherever possible. One such operation can do a lot of computation, and is much faster than doing the same work with a Python loop. Avoid Python loops!

When the vocabulary is large, the slowest part of the forward computation $F_i(\vec{\theta})$ is computing the denominator $Z(xy)$ for each $xy$ that you find, because it involves computing $\tilde{p}(xyz;\vec{\theta})$ for all $z$ and then summing over all of them. You can use fast PyTorch operations for this.\footnote{In principle, it could be helpful to ``memoize" $Z(xy)$ in a dictionary, so that if you see the same $xy$ many times, you don’t have to recompute $Z(xy)$ again each time: you can just look it up. Unfortunately, you \emph{do} still have to recompute $Z(xy)$ if $\vec{\theta}$ has changed since the last time you computed it. Since $\vec{\theta}$ changes often with SGD, this trick may not give a net win.} With efficient coding, each epoch should take about 1 minute on a CPU.

\textit{(Advanced remark:} There are language models that eliminate the need to sum over the whole vocabulary by predicting each word \textit{one bit at a time} \citep{MnihHinton2009}. Then you only have to predict a sequence of $\log_2 V$ bits. Since each prediction is only over two options, the denominator in the probability only has to sum over 2 choices instead of $V$ choices. This is \textit{computationally} faster, but predicting those bits has a somewhat artificial task that’s hard to do \textit{accurately}. Nowadays people seem to just do the $V$-way prediction, ideally using a GPU to accelerate the PyTorch operations.) 

\subsection*{I.4 Make the forward computation correct}

Our probability model $p(z \mid xy)$ predicts the next word $z$ from the vocabulary. Thus, the denominator $Z$ in equation (6) should sum over the vocabulary of possible next words (including EOS and OOV). It should not sum over the lexicon, which is quite different! There might be dozens of vocabulary words that are not in the lexicon: but we still have to add them into $Z$, using the OOL embedding for each of them. Conversely, there might be thousands of lexicon words that are not in the vocabulary: the model simply can’t generate any of them, although it can generate OOV.

To enable fast computation of $Z$, you can precompute a matrix of embeddings of the vocabulary words using \texttt{torch.stack}:
\begin{verbatim}
torch.stack([embedding(word) for word in self.vocab])
\end{verbatim}
where \texttt{embedding(word)} looks up the embedding of a word in the lexicon (returning the OOL embedding if necessary). For this to be correct, \texttt{self.vocab} includes EOS and OOV.

For reasons discussed in reading section G.2, you won’t want to compute $\log p(z \mid xy)$ directly using equation (4). Instead, you’ll want to compute 
\[
\log p(z \mid xy) = \log \tilde{p}(xyz) - \log Z(xy),
\]
where you’d better use \texttt{logsumexp} to help find $\log Z(xy)$. For example, \texttt{torch.logsumexp(torch.Tensor([1,2,3]),0)} or equivalently \texttt{torch.Tensor([1,2,3]).logsumexp(0)} is mathematically equivalent to $\log(\exp(1)+\exp(2)+\exp(3))$, but is faster and more numerically stable.\footnote{When you’re adding up thousands of terms using $\log(\exp(1)+\exp(2)+\exp(3)+\cdots)$, the resulting $\log Z$ may be numerically imprecise—and its gradient will be so imprecise that trying to follow it actually won’t be able to find good parameters for the model!}

What is the reason for the $0$ argument in these expressions? It specifies which dimension of the tensor to operate on. In the example above, we are applying \texttt{logsumexp} to a 1D tensor (vector), so the only possible dimension is 0. But if $A$ is a 2D tensor (matrix), then \texttt{A.logsumexp(0)} sums up each \textit{row} separately, whereas \texttt{A.logsumexp(1)} sums up each \textit{column} separately. This may actually be useful later in the assignment: if you try the mini-batching technique in reading section I.7.6 below, you might be computing $\log Z(xy)$ for many different contexts $xy$ at once.

\subsection*{I.5 Choose your hyperparameters carefully}

The convergence speed of stochastic gradient ascent is sensitive to the initial learning rate $\eta_0$. We recommend trying $\eta_0 = 10^{-2}$ for language ID and $\eta_0 = 10^{-5}$ for spam detection, but you can experiment.

The initial guess $\vec{\theta}^{(0)}$ also matters. The easiest choice is $\vec{\theta}=0$ (i.e., initialize all entries of $X$ and $Y$ to 0), and that will work fine in this case, although it is not recommended in general.\footnote{So what could go wrong, imagine that we were never given the word embeddings in a file, but were learning them along with $X$ and $Y$. Such a model would be symmetric—different dimensions of the embeddings would be governed by the same equations. As a result, if we initialized all parameters to 0, then all parameters of the same kind would have to have identical updates, and therefore, they would continue to have identical values throughout optimization!

The standard solution is to initialize the parameters not to 0, but to random values close to 0—for example, drawn from a normal distribution with mean 0 and small standard deviation. This ``breaks the symmetry" and allows the different dimensions of the word embeddings to specialize and play different roles.}}

(Note that the homework asks you to use the hyperparameters recommended above when \texttt{log\_linear} is selected (question 7(b)). This will let you and us check that your implementation is correct. However, you may want to experiment with different settings, and you are free to use those other settings when \texttt{log\_linear.improved} is selected (see question 7(d)) to get better performance.)

\subsection*{I.6 Compute and apply the gradient properly}

To implement line 7 of Algorithm 1, you should do something like this:
\begin{verbatim}
F_i = log_prob - regularizer  # as defined in (16); depends on θ and on example i
F_i.backward()                # increases θ.grad by the vector (∂F_i/∂θ_1, ∂F_i/∂θ_2, ...)
\end{verbatim}

The first line is the final step of the forward computation, and the second line is the backward computation.

The parameters don’t actually have to be named $\vec{\theta}$. For example, the probability model given by equations (4) to (7) has parameter matrices $X$ and $Y$. Suppose we represent those as 2D tensors $X$ and $Y$, and then use them to help compute $F_i$. Then \texttt{X.grad} and \texttt{Y.grad} will be magically defined to be 2D tensors of the same shapes as $X$ and $Y$. The call to \texttt{F\_i.backward()} serves to modify both of them: for example, it will increase \texttt{(X.grad)[5,2]} by $\frac{\partial F_i}{\partial X_{5,2}}$. To make this happen, however, you need to specify before doing the forward computation that $X$ and $Y$ should track their gradients: either create them with the \texttt{requires\_grad=True} flag, or call their \texttt{requires\_grad\_(true)} method right after you create them.

After doing the backward computation, to complete line 7, you can update $X$ and $Y$ like this:
\begin{verbatim}
with torch.no_grad():
    F += F_i          # keep a running total of F over the epoch, if you like
    X += eta * X.grad # update X in the direction that will increase F_i
    Y += eta * Y.grad # update Y in the direction that will increase F_i
\end{verbatim}

The \texttt{no\_grad} directive says not to track the computations in this block of code, because we don’t need the gradient of \textit{those} computations. It would be very expensive if $F$, $X$, and $Y$ had to remember \textit{all} the steps that computed them! After all, $F$ depends on all of the trigrams in this epoch, and the current values of $X$ and $Y$ depend on all of the updates from all of the epochs so far. If you kept track of all these computations at once, you would probably run out of memory during training. Fortunately, we only need to keep track of the computations for the current $F_i$, not the total $F$. Once we call \texttt{F\_i.backward()}, and take the stochastic gradient step, we are allowed to forget how $F_i$ was computed. This happens automatically because on the next iteration of the loop, $F\_i$ is assigned to a new value. The old value and its attached computation are then no longer accessible (as long as we’re not tracking the computation of $F$), so Python can garbage-collect them when it gets low on memory.

Finally, remember that the \texttt{backward} algorithm doesn’t \textit{set} the \texttt{grad} tensors; it \textit{increases} them, adding to whatever is already there. This is because of how the backprop algorithm works internally: it accumulates an answer into \texttt{grad} by successive addition. Thus, you must explicitly clean up by resetting these accumulators to 0 before the next call to \texttt{backward}.
\begin{verbatim}
X.grad.zero_()
Y.grad.zero_()
\end{verbatim}

The method name \texttt{zero\_} ends in \texttt{\_} because it modifies the tensor in place; this is a PyTorch convention.

\subsection*{I.7 Improve the SGD training loop}

The following improvements are not required for the homework, but they might help you run faster or get better results. You should read this section in any case. Many of these techniques will pay off even more strongly in HW6.

\subsubsection*{I.7.1 Monitor your progress}

Question 7(b) says to print the objective function (14) at the end of each epoch of Algorithm 1. You are free to show additional information using \texttt{log.info()}. You might want to frequently show the objective function, or perhaps its breakdown into log-likelihood and regularizer terms. Better yet, use the \texttt{wandb} Python module to send these numbers to the \href{https://wandb.ai}{Weights \& Biases} website where you can view them on a graph.

The objective should improve reasonably steadily; if not, your stepsize $\eta$ may be too large. You can also show the log-likelihood on dev data; if you are improving on training data yet getting worse on dev data, then you are overfitting to the training data, so your regularization coefficient $C$ may be too small.

\subsubsection*{I.7.2 Manage parameter updates using a PyTorch optimizer}

One annoying thing about the approach of reading section I.6 is that you have to explicitly update both $X$ and $Y$, and you have to explicitly reset both \texttt{X.grad} and \texttt{Y.grad}. The more complex your model, the more variables there are to worry about. The \texttt{torch.nn} module can help with this, if you like:

\begin{itemize}
  \item keep track of all of the model parameters via \texttt{torch.nn.Parameter}
  \item update those parameters via SGD or another optimizer of your choice
\end{itemize}

This is illustrated in the starter code we provided you. \texttt{EmbeddingLogLinearLanguageModel} inherits from \texttt{torch.nn.Module} for this reason. When an instance of this model is created, it registers the tensors $X$ and $Y$ as parameters of the model. Now all of the code in reading section I.6 can be simplified to

\begin{verbatim}
F_i = log_prob - regularizer     # as before
(-F_i).backward()                # note change of sign
optimizer.step()                 # adjusts the parameters (X and Y)
optimizer.zero_grad()            # resets the gradient tensors (X.grad and Y.grad)
\end{verbatim}

The \texttt{optimizer} object manages the state of an optimization algorithm such as SGD. The starter code shows how to create a basic SGD optimizer via \texttt{torch.optim.SGD}. By convention, PyTorch optimizers \emph{minimize} an objective function by taking a step in the direction that will \emph{decrease} the function. That is, \texttt{optimizer} insists on doing stochastic gradient \emph{descent} rather than \emph{ascent}. So instead of maximizing $F = \sum_i F_i$, we will use it to minimize $-F = \sum_i (-F_i)$. That is why we find the gradient of \texttt{-F\_i} above.

\subsubsection*{I.7.3 Convergent SGD}

\texttt{torch.optim.SGD} won’t decrease the learning rate on every step. But decreasing the stepsize in an appropriate way (as in Algorithm 1) is necessary to guarantee that the SGD optimizer eventually converges to a local minimum.

In the special case of log-linear models, our objective function (the negative of equation (14)) happens to be convex, so it has a unique local minimum—namely the global minimum. It’s best to use Algorithm 1 to be sure that you converge to that. Any initial stepsize $\eta_0 > 0$ and decrease rate $\lambda > 0$ will suffice (the choice $\lambda = \tfrac{2C}{N}$ is the one we used in reading section H.2). We have provided an implementation as \texttt{ConvergentSGD}, which you can try as an improvement on \texttt{torch.optim.SGD}. Using its default arguments in the setup of question 7(b), it should achieve $F = -2.9677$ at the end of epoch 10.

(However, in the more general case of deep learning (see Homework 6), the simpler \texttt{torch.optim.SGD} may actually be a better choice, and is widely used. The reason is that the deep learning objective function will not be convex and will have local minima. Some of these are “bad” local minima that only work well on the particular training dataset you used. There is some evidence that keeping a constant learning rate will actually do better at avoiding these bad local minima (i.e., avoiding overfitting), so that the learned parameters generalize better to test data. That is why \texttt{torch.optim.SGD} does not decrease the learning rate.)

\subsubsection*{I.7.4 Deciding when to stop}

In reading section H.2, you may have wondered how to choose $E$, the number of epochs. The homework asks you to use a fixed number of epochs ($E=10$) only to keep things simple. The more traditional SGD approach is to continue running until the function appears to have converged “well enough.” For example, you could stop if the average gradient over the past epoch (or the past $m$ examples) was very small.

In machine learning, our ultimate goal is not actually to optimize the training objective, but rather to do well on test data. Thus, a more common approach in machine learning is to compute the evaluation metric (reading section E) on \emph{development data} at the end of each epoch (or after each group of $m$ examples). Stop if that “dev objective” has failed to improve (say) 3 times in a row. Then you can use the parameter vector $\hat{\theta}$ that performed best on development data. This is known as “early stopping” because SGD may not yet have converged to an optimum on the training objective. Early stopping can be an effective regularizer (especially when $C$ is too small) since it prevents overfitting to the training data. In effect, early stopping treats the number of epochs as a hyperparameter that is tuned on dev data. It’s efficient and effective.

\subsubsection*{I.7.5 Shuffling}

In theory, stochastic gradient descent shouldn’t even use epochs. There should only be one loop, not two nested loops. At each iteration, you pick a random example from the training corpus, and update $\vec{\theta}$ based on that example. Again, you would evaluate on dev data after every $m$ examples to decide when to stop. That’s why it is called “stochastic” (i.e., random). The insight here is that the regularized log-likelihood per token, namely $F(\vec{\theta})$, is actually just the average value of $F_i(\vec{\theta})$ over all of the examples (see (16)). So if you compute the gradient on one example, it is the correct gradient on average (since the gradient of an average is the average gradient). So line 7 is going in the correct direction on average if you choose a random example at each step.

In practice, a common approach to randomization is to still use epochs, so that each example is visited once per epoch, but to \emph{shuffle the examples into a random order} at the start of each epoch (including the first). To see why shuffling can help, imagine that the first half of your corpus consists of Democratic talking points and the second half consists of Republican talking points. If you shuffle, your stochastic gradients will roughly alternate between the two, like alternating between left and right strokes when you paddle a canoe; thus, your average direction over any short time period will be roughly centrist. By contrast, since Algorithm 1 doesn’t shuffle, it will paddle left for the half of each epoch and then right for the other half, which will make significantly slower progress in the desired centrist direction.

\subsubsection*{I.7.6 Mini-batching}

Each step of Algorithm 1 tried to improve $F_i(\vec{\theta})$ for some training example $i$ (in our case, a trigram), by moving the parameters in the direction $\nabla F_i(\vec{\theta})$. But instead, we could choose a “mini-batch” $I$ of several examples (typically the next examples in the shuffled order), and try to improve $\sum_{i \in I} F_i(\vec{\theta})$ by moving the parameters in the direction $\sum_{i \in I} \nabla F_i(\vec{\theta})$.

The advantage of mini-batching is that it breaks the serial dependency of SGD, where each example changes $\vec{\theta}$ for the next example and therefore we can only compute one example at a time. Another way of thinking about it is that a single SGD example now consists of several unrelated trigrams instead of just one. Mini-batching means that the probabilities of all these trigrams are determined using the same current parameter vector $\vec{\theta}$. Their various updates to $\vec{\theta}$ are added together (since the update $\sum_{i \in I} \nabla F_i(\vec{\theta})$ can be regarded as $\sum_{i \in I} \nabla F_i(\vec{\theta})$).

Since all of the trigram probabilities are all computed using the same math operations (but on different data), you can compute them “all at once” using tensor operations. This is often much faster than doing them one at a time, because the tensor operations are implemented in fast low-level C++ and may even exploit hardware speedups—vectorization (on a CPU) or parallelism (on a GPU).

Just take the next several trigrams $xyz,^{20}$ and put all of the $x$ embeddings into one tensor, all of the $y$ embeddings into another, and all of the $z$ embeddings into a third. Then use PyTorch’s tensor operations to compute the features and log-probabilities $\log p(z \mid xy)$ for all of these trigrams at once.

\section*{J Ideas for log-linear features}

Here are some ideas for extending your log-linear model. Most of them are not very hard, although training may be slow. Or you could come up with your own!

Adding features means throwing some more parameters into the definition of the unnormalized probability. For example, extending the definition (7) with additional features (in the case $d=2$) gives

\begin{align}
\tilde{p}(xyz) &\stackrel{\mathrm{def}}{=} \exp\Big(\vec{x}^T X \vec{z} + \vec{y}^T Y \vec{z} + \theta_9 f_9(xyz) + \theta_{10} f_{10}(xyz) + \cdots \Big) \\
&= \exp\Bigg(\underbrace{\theta_1 f_1(xyz) + \cdots + \theta_8 f_8(xyz)}_{\text{as defined in (10)--(11)}} + \theta_9 f_9(xyz) + \theta_{10} f_{10}(xyz) + \cdots \Bigg)
\end{align}

\subsection*{J.1 OOV features}

Some contexts are reasonably likely to be followed by OOV: “I looked up the word \_\_\_\_.” Others are not: “I flew to San \_\_\_\_.” It would be useful to do a good job of predicting $p(\text{OOV} \mid xy)$.

Unfortunately, our basic model (7) does not have the freedom to learn specific parameters for OOV. It just uses its general parameters $X, Y$ with the pre-specified embedding of OOV.

Worse yet, the embedding of OOV is inappropriate. Recall from reading section F.4.1 that when $z=\text{OOV}$, equation (7) takes $\vec{z}$ to be the embedding of \texttt{OOL}, since OOV is not in the lexicon. But that means OOV is treated just like any other out-of-lexicon word. That’s wrong—typically, OOV should have a much higher probability.

probability than any specific out-of-lexicon word that \emph{is} in the vocabulary, because it represents a whole 
class of words: $p(\text{OOV} \mid xy)$ stands for the \emph{total probability} of all words that are out-of-vocabulary. 

Thus, you might add a simple feature $f_{\text{oov}}(xyz)$ that returns 1 if $z = \text{OOV}$ and returns 0 otherwise. 
Given any $xy$, the expression $\vec{x}X\vec{z} + \vec{y}Y\vec{z}$ is still equal for all OOL elements of the vocabulary, since they 
all have the same $\vec{z}$. But when $z = \text{OOV}$, the extra feature $f_{\text{oov}}$ now fires as well, which increases $\hat{p}(xyz)$ 
by a factor of $\exp \theta_{\text{oov}}$. As a result, for any $xy$, $\hat{p}(xyz)$ is exactly $\exp \theta_{\text{oov}}$ times larger when $z = \text{OOV}$ than 
when $z$ is some specific OOL word. The trained value of $\theta_{\text{oov}}$ determines how big this factor is. Roughly 
speaking, this factor should be large if the original training corpus had a lot of different OOV word types. 

You could be fancier and try to learn different OOV parameters for the different dimensions. For example, 
you could add this to the score of $xyz$ before you exponentiate: 

\[
\begin{cases}
\vec{x}\cdot \vec{x}_{\text{oov}} + \vec{y}\cdot \vec{y}_{\text{oov}} & \text{if } z = \text{OOV} \\
0 & \text{otherwise}
\end{cases} \tag{25}
\]

Here $\vec{x}_{\text{oov}}$ and $\vec{y}_{\text{oov}}$ are learned parameter vectors, so in effect we are learning $2d$ additional feature weights. 
This should do a better job of learning how much to raise (or lower) the probability of $z = \text{OOV}$ according 
to the various properties of the context words $x$ and $y$. Convince yourself that this model is still log-linear.\footnote{An alternative would be to replace OOV’s embedding $\vec{z}$ with a new, learned embedding vector $\vec{z}_{\text{oov}}$. Is that still log-linear? Well, if we only use the new embedding in the $z$ position, then we’re changing the score of $xyz$ from $\vec{x}^TX\vec{z} + \vec{y}^TY\vec{z}$ to $\vec{x}^TX\vec{z}_{\text{oov}} + \vec{y}^TY\vec{z}_{\text{oov}}$. But we can get exactly the same effect with the scheme above by learning $\vec{x}_{\text{oov}} = X(\vec{z}_{\text{oov}} - \vec{z})$ and $\vec{y}_{\text{oov}} = Y(\vec{z}_{\text{oov}} - \vec{z})$.}

\subsection*{J.2 Unigram log-probability}

More generally, a weakness of the model (7) is that it doesn’t have any parameters that keep track of how 
frequent specific words are in the training corpus! Rather, it backs off from the words to their embeddings. 
Its probability estimates are based \emph{only} on the counts of embeddings, which were learned from some other (larger) 
corpus. 

It’s pretty common in NLP to use SGD to adjust the embeddings at the same time as the other parameters. 
But then we wouldn’t have a log-linear model anymore, as discussed below equation (11). 

One way to fix the weakness while staying within the log-linear framework would be to have a binary 
feature $f_w$ for each word $w$ in the vocabulary, such that $f_w(xyz)$ is 1 if $z=w$ and 0 otherwise. We’ll do 
that in reading section J.3 below. 

But first, here’s a simpler method: just add a single \emph{non-binary} feature defined by 

\[
f_{\text{unigram}}(xyz) = \log \hat{p}_{\text{unigram}}(z) \tag{26}
\]

where $\hat{p}_{\text{unigram}}(z)$ is estimated by add-1 smoothing. Surely we have enough training data to learn an appro-
priate weight for this \emph{single} feature. In fact, because \emph{every} training token $w_i$ provides evidence about this 
single feature, its weight will tend to converge quickly to a reasonable value during SGD. 

This is not the only feature in the model—as usual, you will use SGD to train the weights of \emph{all} features 
to work together, computing the gradient via (18). Let $\beta = \theta_{\text{unigram}}$ denote the weight that we learn for 
the new feature. By including this feature in our definition of $\hat{p}_{\text{unigram}}(z)$, we are basically multiplying a 
factor of $(\hat{p}_{\text{unigram}}(z))^\beta$ into the numerator $\hat{p}(xyz)$ (check (5) to see that this is true). This means that in the 
special case where $\beta = 1$ and $X = Y = 0$, we simply have $\hat{p}(xyz) = \hat{p}_{\text{unigram}}$, so that the log-linear model 
gives exactly the same probabilities as the add-1 smoothed unigram model $\hat{p}_{\text{unigram}}$. However, by training 
the parameters, we might learn to trust the unigram model less ($0 < \beta < 1$) and rely more on the word 
embeddings ($X, Y \neq 0$) to judge which words $z$ are likely in the context $xy$. 

A quick way to implement this scheme is to define 

\[
f_{\text{unigram}}(xyz) = \log(c(z) + 1) \quad \text{(where $c(z)$ is the count of $z$ in training data)} \tag{27}
\]

This gives the same model, since $\hat{p}_{\text{unigram}}(z)$ is just $c(z)+1$ divided by a constant, and our model renormal-
izes $\hat{p}(xyz)$ by a constant anyway. 

\subsection*{J.3 Unigram, bigram, and trigram indicator features}

Try adding a unigram feature $f_w$ for each word $w$ in the vocabulary. That is, $f_w(xyz)$ is 1 if $z=w$ and 0 
otherwise. Does this work better than the log-unigram-probability feature from reading section J.2? 

Now try also adding a binary feature for each bigram and trigram that appears at least 3 times in training 
data. How good is the resulting model? 

In all cases, you will want to tune $C$ on development data to prevent overfitting. This is important—the 
original model had only $2d^2 + 1$ parameters where $d$ is the dimensionality of the embeddings, but your new 
model has enough parameters that it can easily overfit the training data. In fact, if $C = 0$, the new model 
will \emph{exactly} predict the unsmoothed probabilities, as if you were not smoothing at all (add-0)! The reason 
is that the maximum of the concave function $F(\vec{\theta}) = \sum_{i=1}^N F_i(\vec{\theta})$ is achieved when its partial derivatives 
are 0. For each unigram feature $f_w$ defined in the previous paragraph, we have, from equation (18) with 
$C = 0$, 

\[
\frac{\partial F(\vec{\theta})}{\partial \theta_w} = \sum_{i=1}^N \frac{\partial F_i(\vec{\theta})}{\partial \theta_w} \tag{28}
\]

\[
= \sum_{i=1}^N f_w(xyz) \;-\; \sum_{i=1}^N \sum_{z'} \hat{p}(z' \mid xy) f_w(xyz') \tag{29}
\]

Hence SGD will adjust $\vec{\theta}$ until this is 0, that is, until the predicted count of $w$ exactly matches the observed 
count $c(w)$. For example, if $c(w) = 0$, then SGD will try to allocate 0 probability to word $w$ in all contexts 
(no smoothing), by driving $\theta_w \to -\infty$. Taking $C > 0$ prevents this by encouraging $\theta_w$ to stay close to 0. 

\subsection*{J.4 Embedding-based features on unigrams and trigrams}

Oddly, (7) only includes features that evaluate the bigram $yz$ (via weights in the $Y$ matrix) and the skip-
\emph{bigram} $xz$ (via weights in the $X$ matrix). After all, you can see in (9) that the features have the form $y_j z_m$ 
and $x_j z_m$. This seems weaker than add-$\lambda$ with backoff. Thus, add unigram features of the form $z_m$ and 
trigram features of the form $x_h y_j z_m$. 

\subsection*{J.5 Embedding-based features based on more distant skip-bigrams}

For a log-linear model, there’s no reason to limit yourself to trigram context. Why not look at 10 previous 
words rather than 2 previous words? In other words, your language model can use the estimate $p(w_i \mid w_{i-10}, w_{i-9}, \ldots w_{i-1})$. 

There are various ways to accomplish this. You may want to reuse the $X$ matrix at all positions $i-10, i-9, \ldots, i-2$ (while still using a separate $Y$ matrix at position $i-1$). This means that having the 
word “bread” anywhere in the recent history (except at position $w_{i-1}$) will have the same effect on $w_i$. Such 
a design is called “tying” the feature weights: if you think of different positions having different features 
associated with them, you are insisting that certain related features have weights that are “tied together” 
(i.e., they share a weight). 

You could further improve the design by saying that “bread” has weaker influence when it is in the more 
distant past. This could be done by redefining the features: for example, in your version of (9), you could 
scale down the feature value $(x_j z_m)$ by the number of word tokens that fall between $x$ and $z$.

\textbf{Note:} The provided code has separate methods for 3-grams, 2-grams, and 1-grams. To support general 
$n$-grams, you’ll want to replace these with a single method that takes a list of $n$ words. It’s probably easiest 
to streamline the provided code so that it does this for all smoothing methods. 

\subsection*{J.6 Spelling-based features}

The word embeddings were automatically computed based on which words tend to appear near one another. 
They don’t consider how the words are \emph{spelled}! So, augment each word’s embedding with additional di-
mensions that describe properties of the spelling. For example, you could have dimensions that ask whether 
the word ends in -\emph{ing}, -\emph{ed}, etc. Each dimension will be 1 or 0 according to whether the word has the 
relevant property. 

Just throw in a dimension for each suffix that is common in the data. You could also include properties 
relating to word length, capitalization patterns, vowel/consonant patterns, etc.—anything that you think 
might help! 

You could easily come up with thousands of properties in this way. Fortunately, a given word such as 
\emph{burgeoning} will have only a few properties, so the new embeddings will still be \emph{sparse}. That is, they consist 
mostly of 0’s with a few nonzero elements (usually 1’s). This situation is very common in NLP. As a result, 
you don’t need to store all the new dimensions: you can compute them on demand when you are computing 
summations like $\sum_{j=1}^d \sum_{m=1}^M Y_{jm}\cdot(y_j z_m)$ in (9). In such a summation, $j$ ranges over possible suffixes of 
$y$ and $m$ ranges over possible suffixes of $z$ (among other properties, including the original dimensions). To 
compute the summation, you only have to loop over the few dimensions $j$ for which $y_j \neq 0$ and the few 
dimensions $m$ for which $z_m \neq 0$. (All other summands are 0 and can be skipped.) 

It is easy to identify these few dimensions. For example, \emph{burgeoning} has the -\emph{ing} property but 
not any of the other 3-letter-suffix properties. In the trigram $xyz = \text{demand was burgeoning}$, the 
summation would include a feature weight $Y_{jm}$ for $j = -\text{was}$ and $m = -\text{ing}$, which is included because 
$yz$ has that particular pair of suffixes and so $y_j z_m = 1$. In practice, $Y$ can be represented as a hash map 
whose keys are pairs of properties, such as pairs of suffixes.

\subsection*{J.7 Meaning-based features}

If you can find online dictionaries or other resources, you may be able to obtain other, linguistically inter-
esting properties of words. You can then proceed as with the spelling features above.

\subsection*{J.8 Repetition}

Since words tend to repeat, you could have a feature that asks whether $w_i$ appeared in the set $\{w_{i-10}, w_{i-9}, \ldots, w_{i-1}\}$. 
This feature will typically get a positive weight, meaning that recently seen words are likely to appear again. 
Since 10 is arbitrary, you should actually include similar features for several different history sizes: for 
example, another feature asks whether $w_i$ appeared in $\{w_{i-20}, w_{i-19}, \ldots w_{i-1}\}$. 

Of course, this is no longer a trigram model, but that’s ok! 

\subsection*{J.9 Ensemble modeling}

Recall that equation (26) included the log-probability from another model as a feature within your log-linear 
model. You could include other log-probabilities in the same way, such as smoothed bigram and trigram 
probabilities from question 5. The log-linear model then becomes an “ensemble model” that combines the 
probabilities of several other models, learning how strongly to weight each of these other models. 

If you want to be fancy, your log-linear model can include various trigram-model features, each of 
which returns $\log \hat{p}_{\text{trigram}}(z \mid xy)$ but only when $c(xy)$ falls into a particular range, and returns 0 otherwise. 
Training might learn different weights for these features. That is, it might learn that the trigram model is 
trustworthy when the context $xy$ is well-observed, but not when it is rarely observed. 


\end{document}

