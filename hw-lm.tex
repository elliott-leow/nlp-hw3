\documentclass[12pt]{article}

% ========== PACKAGES ==========
\usepackage[utf8]{inputenc}      % UTF-8 support
\usepackage[T1]{fontenc}         % Better font encoding
\usepackage{lmodern}             % Latin Modern font
\usepackage{amsmath, amssymb}    % Math environments & symbols
\usepackage{amsthm}              % Theorem/proof environments
\usepackage{mathtools}           % Math refinements
\usepackage{physics}             % Common physics/math macros
\usepackage{graphicx}            % Figures
\usepackage{float}               % Improved float placement
\usepackage{enumitem}            % Customizable lists
\usepackage{geometry}            % Page layout
\usepackage{hyperref}            % Hyperlinks
\usepackage{xcolor}              % Colored text if needed

% ========== PAGE SETUP ==========
\geometry{margin=1in}

% ========== CUSTOM THEOREM ENVIRONMENTS ==========
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% ========== DOCUMENT ==========
\documentclass[12pt]{article}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{amssymb}

\setlist[itemize]{leftmargin=2em}
\setlist[enumerate]{leftmargin=2em}

\title{601.465/665 --- Natural Language Processing\\
Homework 3: Smoothed Language Modeling}
\author{Prof. Jason Eisner --- Fall 2025}
\date{Due date: Sunday 5 October, 11 pm}

\begin{document}
\maketitle

\section*{Introduction}
Probabilistic models are an indispensable part of modern NLP. This homework will try to convince you that even simplistic and linguistically stupid models like $n$-gram models can be useful, provided their parameters are estimated carefully. See section A in the attached reading handout.

You now know enough about probability to build and use some trigram language models. You will experiment with different types of smoothing, including using PyTorch to train a log-linear model. You will also get some experience in running corpus experiments over training, development, and test sets. This is the only homework in the course to focus on that.

\section*{Homework goals}
After completing this homework, you should be comfortable with
\begin{itemize}
    \item estimating conditional probabilities from supervised data
    \begin{itemize}
        \item direct estimation of probabilities (with simple or backoff smoothing)
        \item conditional log-linear modeling (including feature engineering using external information such as lexicons)
    \end{itemize}
    \item subtleties of language modeling (tokenization, \texttt{EOS}, \texttt{OOV}, \texttt{OOL})
    \item subtleties of training (logarithms, autodiff, SGD, regularization)
    \item evaluating language models via sampling, perplexity, and multiple tasks, using a train/dev/test split
    \item tuning hyperparameters by hand to improve a formal evaluation metric
    \item implementing these methods cleanly in Python
    \begin{itemize}
        \item partitioning the works sensibly into different classes, files, and scripts that play nicely together
        \item using basic facilities of PyTorch
        \item using remote GPUs to speed up your computation (if you choose)
    \end{itemize}
\end{itemize}

\newpage
\section{Perplexities and corpora}
Your starting point is the sample programs \texttt{build.vocab.py} \texttt{build.lm.py} \texttt{fileprob.py}, in the code directory. The \texttt{INSTRUCTIONS} file in the same directory explains how to get the programs running (e.g., exactly what to type). Those instructions will let you automatically compute the $\log_2$-probability of three sample files (\texttt{data/speech/\{sample1,sample2,sample3\}}). Try it!

More precisely, for each file, \texttt{fileprob} will give you the total $\log_2$-probability of all token sequences in the file. Each line of the file is considered to be a separate token sequence (sentence or document) that is implicitly preceded by \texttt{BOS} and followed by \texttt{EOS}.

Next, you should spend a little while studying those sample programs yourself, and browsing around the \texttt{data/} directory to see what’s there. See reading sections B and C for more information.

\begin{enumerate}[label=\reflectbox{\textifsymbol{34}}\arabic*]
    \item If a language model is built from the \texttt{data/speech/switchboard-small} corpus, using add-0.01 smoothing and a vocab threshold of 3, what is the model’s \textit{perplexity per word} on each of the three sample files?
    \item What happens to the log$_2$-probabilities and perplexities if you train instead on the larger \texttt{switchboard} corpus? Why?
\end{enumerate}

\newpage
\section{Implementing a generic text classifier}
Modify \texttt{fileprob} to obtain a new program \texttt{textcat} that does text categorization via Bayes’ Theorem.

The two programs both use the same \texttt{probs} module to get the language model probabilities. So when you extend \texttt{probs.py} with new smoothing methods in question 5 below, they will immediately be available from both programs.

\texttt{textcat} should be run from the command line almost exactly like \texttt{fileprob}. However,
\begin{itemize}
    \item it needs to take \textit{two} language models, one for each category of text
    \item it needs to specify the prior probability of the first category
\end{itemize}

Of course, you can imagine extending this to do $n$-way classification, with $n$ language models each trained on a different corpus, and prior probabilities for each of the $n$ categories. But in this homework, we will stick with binary classification.

Again, consult \texttt{INSTRUCTIONS} for some tips on working with the starter code. You’ll train models of genuine emails (\texttt{gen}) and spam emails (\texttt{spam}). You (and the graders) should then be able to use your program for classification like this:
\begin{verbatim}
./textcat.py gen.model spam.model 0.7 foo.txt bar.txt baz.txt
\end{verbatim}

which uses the two trained models to classify the listed files. Its printed output should label each file with a name (in this case \texttt{gen} or \texttt{spam}):

\begin{verbatim}
spam.model   foo.txt
spam.model   bar.txt
gen.model    baz.txt
1 files were more probably from gen.model (33.33%)
2 files were more probably from spam.model (66.67%)
\end{verbatim}

In other words, your program classifies each file by printing its maximum \textit{a posteriori} class (the file name of the model that more probably generated it). Then it prints a summary of all the files.

The number 0.7 on the above command line specifies your prior probability that a test file will be \texttt{gen}. Thus, 0.3 is your prior probability that it will be \texttt{spam}. See reading section C.2.

Please use the exact output formats above. If you would like to print any additional output lines for your own use, please direct it to STDERR, using the logging facility as illustrated in the starter code.

As reading section D.3 explains, both language models that you provide to \texttt{textcat} should use the same finite vocabulary. Specifically, please construct this vocabulary to consist of words that appeared $\geq 3$ times in the union of the gen and spam training corpora, plus OOV and EOS.\footnote{Of course, OOV and EOS may appear in the training corpora too—in fact, EOS must appear. But they might appear $<3$ times. \texttt{build.vocab} is careful to include them in the vocabulary anyway. This is important. If \texttt{OOV} weren’t in the vocabulary, we wouldn’t be able to handle OOV words. And if EOS weren’t in the vocabulary (and hence was treated as just part of the OOV category), then we couldn’t sample from the language model—we wouldn’t know when we had generated EOS and could end the sentence!}

To check your work: When we trained on the smallest training sets with $\lambda=1$ and then classified all 270 dev files with a prior $p(gen)=0.7$, just as in question 3(a) below, we classified 23 of them as \texttt{spam}.
 
\newpage
\section{Evaluating a text classifier}

In this question, you will evaluate your \texttt{textcat} program on the problem of spam detection. The datasets are under \texttt{data/gen\_spam}. Look at the \texttt{README} file there, and then examine the training data to get a sense for how the genuine emails differ from the spam emails. (\textbf{Don’t peek at the test data!})

Using add-1 smoothing, run \texttt{textcat} on all the dev data for your chosen task. That is, train your language models on the gen and spam training sets, and then classify the files \texttt{gen\_spam/dev/gen/*} and \texttt{gen\_spam/dev/spam/*}. Use 0.7 as your prior probability of \texttt{gen}.

\begin{enumerate}[(a)]
\item From the results, you should be able to compute a total error rate for the technique. That is, what percentage of the dev files were classified incorrectly? (See reading section E.)

\item[\textbf{(b) Extra credit:}] We will focus on the spam detection problem in this assignment. But lectures in class focused on the language identification task, using a character-trigram model instead of a word-trigram model. Formally, these settings are very similar. If you’re curious, you can try out the language ID setting as well, using the data in \texttt{data/english\_spanish}. This should be quite fast since the corpora and vocabulary are small. Train your language models on the \texttt{en.1K} and \texttt{sp.1K} datasets, then classify the files \texttt{english-spanish/dev/english/*/*} and \texttt{english-spanish/dev/spanish/*/*}. Use 0.7 as your prior probability of English. All of these files have been tokenized into characters for you, so that you can use the same code as before.\footnote{Properly speaking, these sequences are not complete sentences. They are substrings plucked from the middle of documents, so they don’t really have BOS or EOS. Ideally, you would change the iterator over trigrams to reflect that: if you observe the sequence $w_1w_2w_3w_4$, the only trigram probabilities that you can compute or train are $p(w_3 \mid w_1w_2)$, $p(w_4 \mid w_2w_3)$, and $p(w_5 \mid w_3w_4)$, because you don’t know what was before $w_1$ (not necessarily BOS) or after $w_5$ (not necessarily EOS).} 

What percentage of the dev files were classified incorrectly?

\item How small do you have to make the prior probability of \texttt{gen} before \texttt{textcat} classifies all the dev files as \texttt{spam}?

\item Now try add-$\lambda$ smoothing for $\lambda \neq 1$. First, use \texttt{fileprob} to experiment by hand with different values of $\lambda > 0$. (You’ll be asked to discuss in question 4(b) why $\lambda = 0$ probably won’t work well.)

What is the minimum cross-entropy per token that you can achieve on the \texttt{gen} dev files (when estimating a model from gen training files with add-$\lambda$ smoothing)? How about for \texttt{spam}?

In principle, you should try a lot of $\lambda$ values to find the one that does best on dev data.\footnote{Perhaps using the charming little \textit{golden-section search algorithm}, a lovely approach when you only have one hyperparameter ($\lambda$). When you have multiple hyperparameters, it is typical to use grid search, Nelder-Mead, or a randomized search algorithm.} However, for purposes of this assignment, it’s okay to try just a few $\lambda \in \{5, 0.5, 0.05, 0.005, 0.0005\}$.

\item In principle, you should apply different amounts of smoothing to the \texttt{gen} and \texttt{spam} models. For example, if \texttt{gen}’s dev set has a higher rate of novel words than \texttt{spam}’s dev set, then you’d want to smooth \texttt{gen} more.

However, for simplicity in this homework, let’s smooth both models in exactly the same way. So what is the minimum cross-entropy per token that you can achieve on all development files together, if both models are smoothed with the same $\lambda \in \{5, 0.5, 0.05, 0.005, 0.0005\}$?

(As in the previous question, you should be evaluating the \texttt{gen} model on \texttt{gen} development files only, and the \texttt{spam} model on \texttt{spam} development files only, to make sure that they are good models of their intended categories. To measure the overall cross-entropy per token for a given $\lambda$, find the \textit{total} number of bits that it takes to predict \textit{all} of the development files from their respective models. This means running \texttt{fileprob} twice: once for the \texttt{gen} data and once for the \texttt{spam} data. Add the two results, and then divide by the \textit{total} number of tokens in all of the development files.)

\item What value of $\lambda$ gave you this minimum cross-entropy? Call this $\lambda^*$. (See reading section E for why you are using cross-entropy to select $\lambda^*$.)

\item Each of the dev files has a length. The length in words is embedded in the filename (as the first number). Come up with some way to quantify or graph how the performance of add-$\lambda^*$ (on dev data) depends on file length. You can decide how to measure performance.

Some tips about graphing are here. A raw scatterplot may be a little hard to interpret, so you could try binning the file lengths to make a histogram, or fit a trend line using lowess (loess) smoothing. Show your graphs and write up your findings.

\item[\textbf{(g) Extra credit:}] If you are also experimenting with language ID, make the same graph for those experiments. The length in words is again embedded in the filename (as the first number), and also appears in the directory name.

\item Now try increasing the amount of \texttt{training} data. (Keep using add-$\lambda^*$, for simplicity.) Compute the overall error rate on dev data for training sets of different sizes: \texttt{gen vs. spam; gen-times2 vs. spam-times2} (twice as much training data); and similarly for \texttt{...-times4} and \texttt{...-times8}.

Graph the error rate (on the $y$ axis) versus the training data size (on the $x$ axis). This is sometimes called a “learning curve.” Do you expect error rate to approach 0 as training size $\rightarrow \infty$?

\item[\textbf{(i) Extra credit:}] If you’re also experimenting with language ID, you can do the same exercise there if you’re still curious. We’ve provided training corpora of 6 sizes: \texttt{en.1K vs. sp.1K} (1000 characters each); \texttt{en.2K vs. sp.2K} (2000 characters each); and similarly for \texttt{5K, 10K, 20K, and 50K}.
\end{enumerate}

\section{Analysis}

Reading section F gives an overview of several smoothing techniques beyond add-$\lambda$.

\begin{enumerate}[(a)]
\item At the end of question 2 and in reading section D.4, the vocabulary size $V$ was carefully defined to include OOV. So if you saw 19,999 different word types in training data, then $V=20{,}000$. What would go wrong with the \texttt{UNIFORM} estimate if you mistakenly took $V=19{,}999$? What would go wrong with the add-$\lambda$ estimate?

\item What would go wrong with the add-$\lambda$ estimate if we set $\lambda = 0$? You can even try it! (Remark: This gives an unsmoothed “relative frequency estimate.” It is commonly called the \textit{maximum-likelihood estimate}, because it maximizes the probability of the \textit{training} corpus.)

\item Let’s see on paper how backoff behaves with novel trigrams. If $c(xyz) = c(xyz') = 0$, then does it follow that $\hat{p}(z \mid xy) = \hat{p}(z' \mid xy)$ when those probabilities are estimated by smoothing? In your answer, work out and state the value of $\hat{p}(z \mid xy)$ in this case. How do these answers change if $c(xyz) = c(xyz') = 1$?

\item In add-$\lambda$ smoothing with backoff, how does increasing $\lambda$ affect the probability estimates? (Think about your answer to the previous question.)
\end{enumerate}

\section{Backoff smoothing}

Implement add-$\lambda$ smoothing with backoff, as described in reading section F.3. This should be just a few lines of code. You will only need to understand how to look up counts in the hash tables. Just study how the existing methods do it.

\textit{Hint:} So $\hat{p}(z \mid xy)$ should back off to $\hat{p}(z \mid y)$, which should back off to $\hat{p}(z)$, which backs off to \ldots what? Figure it out! Think back to the Tablish language from recitation.

You can test out the method as you see fit, including experimenting with different $\lambda$ values.

\textbf{Note:} Feel free to describe your findings in your writeup, but there is nothing to hand in here other than your code. To check your code, the autograder will run it on small training and testing files with some $\lambda$.

\section{Sampling from language models}

So far, we have used our language models to compute the probability of given word sequences. Each language model represents a probability distribution over word sequences.

But because these are well-defined probabilistic models, we can also \textit{sample} from the distributions they represent. As we saw in class, we sample random text by rolling a sequence of weighted dice whose sides are words. This is a good way to see what the language model does and doesn’t know about English.

This is just like Homework 1, where your PCFG allowed you both to sample a new sentence (\texttt{randsent}) and to compute the probability of a given sentence (\texttt{parse}). You can also do both of these things with an $n$-gram model, which is a different generative model of text.\footnote{Actually, not so different: an $n$-gram model turns out to be a special case of a PCFG. Can you show that this is true for $n=2$?}

Implement a generic sampling method that will work with any of our trained language models. When called, your method should condition on \texttt{BOS} and produce new tokens until it reaches \texttt{EOS}. These tokens are drawn from the smoothed model distribution according to their probabilities. This should remind you of \texttt{randsent} in Homework 1. Note that OOV will sometimes be drawn, since it will have positive probability.

Your method should not return or yield \texttt{BOS} and \texttt{EOS}, since they are not part of the sampled sentence. They are just part of an $n$-gram model’s particular sampling process (similar to private variables in a class implementation). If instead you were generating sentences with a PCFG model, then you would start with \texttt{ROOT} at the top rather than \texttt{BOS} at the left, and you would stop at terminal symbols at the bottom rather than needing to generate \texttt{EOS} at the right.

Write a separate script based on \texttt{fileprob} that will sample $k$ sentences from a given language model, using the sampling method you described above. (Especially because of \texttt{UNIFORM}, impose a maximum length limit $M$, as in the PCFG homework. Sequences longer than your configurable limit should be truncated with ``\texttt{...}'')

Since you’re using PyTorch, you should probably sample using \texttt{torch.multinomial} (rather than the Python \texttt{random} module that you probably used in HW1).

We should be able to call your script like this:
\begin{verbatim}
./trigram_randsent.py model_file 10 --max_length 20
\end{verbatim}

\begin{enumerate}[(a)]
\item Choose two trained models that seem to have noticeably different behavior. (They might use different smoothing methods, or different hyperparameters.) Give a sample of 10 sentences from each of the models. Discuss the differences you see and why they arise.
\end{enumerate}

\section{Implementing a log-linear model and training it with backpropagation}

\begin{enumerate}[(a)]
\item Add support for a log-linear trigram model. This is another smoothed trigram model, but the smoothing comes from several feature functions instead of modified or backed-off counts. As usual, see \texttt{INSTRUCTIONS} for details about working with the starter code.

Your code will need to compute $\hat{p}(z \mid xy)$ using the specific features in reading section F.4.1. The parameters $\vec{\theta}$ will be stored in $X$ and $Y$ matrices. You can use random or zero parameters at first, just to get the code working.

Remember that you need to look up an embedding for each word, falling back to the \texttt{OOL} embedding if that word is not in the lexicon. In particular, OOV will fall back to \texttt{OOL}.

You can use embeddings of your choice from the \texttt{lexicons} directory. (See the \texttt{README} file in that directory.) These word embeddings were derived from Wikipedia, a large diverse corpus with lots of useful evidence about the usage of many English words.

Make sure to use character embeddings if you try \texttt{english/spanish}. For \texttt{gen/spam}, you should use word embeddings; \texttt{words-gs-only-*} is recommended based on our experiments.

\item Implement a function that uses stochastic gradient descent to find the $X$ and $Y$ matrices in equation (7) that minimize $-F(\vec{\theta})$, which is the $L_2$-regularized objective function described in reading section H.1. (This is equivalent to maximizing $F(\vec{\theta})$.)

If you initialize the matrices to 0 (see reading section I.5) (so $\vec{\theta}=\vec{0}$), then you are initializing to a uniform distribution. So it is a good sanity check that training for 0 epochs (\texttt{--epochs 0}) in fact gives the same results as just using a uniform distribution (\texttt{--smoother uniform}).

You may prefer to try log-linear models first on language ID (\texttt{data/english\_spanish}), since training a log-linear model takes significantly more time than add-$\lambda$ smoothing. For example, here’s what you should get if you train a log-linear language model for 10 epochs on \texttt{en.1k}, with a vocabulary of size 30 derived from \texttt{en.1k} with threshold 3, the features described in reading section F.4, the character embeddings of dimension $d=10$ (\texttt{chars-10.txt}), $L_2$ regularization with coefficient $C=1$, and the \texttt{torch.optim.SGD} optimizer with its default arguments (see reading section I.7.2) and learning rate $\eta=0.01$:

\begin{verbatim}
Training from corpus en.1K
epoch 1: F = -3.2130978107452393
epoch 2: F = -3.0860429996185303
epoch 3: F = -3.037041425704956
... [you should print these epochs too]
epoch 10: F = -2.94611166000366
Finished training on 1027 tokens
\end{verbatim}

You may find it helpful to speed this up by using a GPU as explained in reading section K. Learning how to do that will come in handy in question 7(d) below and again in Homework 6.

\item You should now be able to measure cross-entropies and text categorization error rates under your fancy new language model! \texttt{textcat} should work as before. Just construct two log-linear models over a shared vocabulary, and then compare the probabilities of a new document (dev or test) under these models.

For the autograder’s sake, when \texttt{log-linear} is specified on the command line, please train for $E=10$ epochs, use the exact hyperparameters suggested in reading section I.5, use the \texttt{torch.optim.SGD} optimizer (reading section I.7.2), and print output in the format above (this is printing $F(\vec{\theta})$ rather than $F_i(\vec{\theta})$). Your training numbers should be close to what we got, although we won’t expect them to match perfectly.

\begin{enumerate}
\item[19.] Report cross-entropy on \texttt{gen\_spam} with $C=1$. Then experiment with other values of $C>0$. Also experiment with the different lexicons, where $d$ is the embedding dimensionality. Report your best results and the hyperparameters that achieved them. For this homework, it’s enough to identify a pretty good value of $C$ when $d=10$—call it $C^*$—and then hold $C^*$ fixed while trying different lexicons. For this homework, it’s also enough to try just $C \in \{0, 0.1, 0.5, 1, 5\}$ and $d \in \{10, 50, 200\}$, and to use only the \texttt{gs-only} lexicons.\footnote{If you wanted to get a more precise $C^*$, you could use golden section search (footnote 4) over $C \in \mathbb{R}$. And if you wanted to look at more hyperparameter combinations, you could try all hyperparameter vectors in the 2-dimensional ``grid'' $(C,d) \in \{0,0.1,0.5,1,5\} \times \{10,50,200\}$. This common brute-force strategy is known as ``grid search.'' Some other strategies for optimizing a hyperparameter vector were mentioned in footnote 4 \ldots but let’s not spend all week on this homework.}

Did $C$ matter a lot? Why or why not? (\textit{Hint:} Regularization helps when you have many rare features that could otherwise be freely used to overfit the model to the specific training examples.)

How does your cross-entropy with $C^*$ compare to that of the add-$\lambda$ models? Why do you think that is? (\textit{Hint:} Again consider your feature set, which was drawn from reading section F.4.1. You will have a chance to expand the feature set in the next question, 7(d).)

\item[20.] Finally, using your lowest-cross-entropy model, measure the classification error rate. How much can you improve this by adjusting the prior probability $p(\textit{gen})$? (Although 62\% of training examples and 67\% of dev examples are \textit{gen}, you’ll find that other values work better.)

\item[21.] How and when did you use the training, development, and test data in all of this optimization? Why do you think you needed to set $p(\textit{gen})$ so large in order to get the system to classify enough documents as \textit{gen}? How do your results compare to add-$\lambda$ backoff smoothing?
\end{enumerate}

\item Now you get to have some fun! Add some new features to the log-linear model and report the effect on its performance. In fact, this may be necessary to beat the simpler add-$\lambda$ methods. Some possible features are suggested in reading section J. \textit{You should make at least one non-trivial improvement}; you can do more for extra credit, including varying hyperparameters and training protocols (reading sections I.5 and I.7).

A good way to devise features is to try sampling sentences from the basic log-linear model (using your \texttt{sample} method from question 6). What’s wrong with these sentences? Specifically, what’s wrong with the trigrams, since that’s all that you can fix within the limits of a trigram model? Are there features that you think are too frequent, or not frequent enough? If so, try adding those features, and then the trained model should get them to occur at the right rate. (Remember from the log-linear visualization that the predictions of a log-linear model, if it was trained without regularization, will have the same features on average as actual words in the training corpus.)

For this question, you can additionally try changing the optimization method (see reading section I.7) and the hyperparameters in search of better results.

Your improved method should be selected with the command-line argument \texttt{log-linear.improved} (in place of \texttt{add.lambda}, \texttt{log.linear}, etc.). You will submit your code and your trained model to Gradescope for autograding.

You are free to submit many versions of your system—with different implementations of \texttt{log-linear.improved}. All will show up on the Gradescope leaderboard, so that you and your classmates can see what works well. (You should submit only \texttt{log-linear.improved} models to the leaderboard, not the other kinds of model, even if those do better.) For final grading, the autograder will take the submitted version of your system that worked best on the released \texttt{test} data, and then evaluate its performance on \texttt{grading-test} data.
\end{enumerate}

\section{Speech recognition}

Finally, we turn briefly to speech recognition. In this task, instead of choosing the best model for a given string, you will choose the best string for a given model.

The data are in the \texttt{speech} subdirectory, drawn from the Switchboard corpus (see the \texttt{README} file there). As usual, it is divided into training, development, and test sets. Here is a sample file (\texttt{dev/easy/easy025}):

\begin{verbatim}
8   i found that to be *hesitation very helpful
0.375   -3524.815668181726  8   i found that the uh it’s very helpful
0.250   -3517.19721540798   9   i found that to be a very helpful
0.125   -3517.19721540798   8   i found that to be a very helpful
0.375   -3524.07213817617   9   oh i found out to be a very helpful
0.375   -3521.50317920669   9   i’ve found out to be a very helpful
0.375   -3525.89574087085   8   but i found out to be a very helpful
0.250   -3515.72595763771   8   i’ve found that to be a very helpful
0.125   -3517.19721540798   8   i found that to be a very helpful
0.500   -3513.58278343221   7   i’ve found that’s be a very helpful
\end{verbatim}

Each file has 10 lines and represents a single audio-recorded utterance $u$. The first line of the file is the correct transcription, preceded by its length in words. The remaining 9 lines are some of the possible transcriptions that were considered by a speech recognition system—including the one that the system actually chose to output. Let’s reason about how to choose among the 9 candidates.

Consider the last line of the sample file. The line shows a 7-word transcription $\vec{w}$ surrounded by sentence delimiters $\langle s \rangle \ldots \langle /s \rangle$ and preceded by its length, namely 7. The number $-3513.58$ was the speech recognizer’s estimate of $\log p(u \mid \vec{w})$: that is, if someone really were trying to say $\vec{w}$, what is the log-probability that it would have come out of their mouth sounding like $u$?\footnote{Actually, the real estimate was 15 times as large. Noisy-channel speech recognizers are really rather bad at estimating $\log p(u \mid \vec{w})$, so they all use a horrible hack of dividing this value by about 15 to prevent it from influencing the choice of transcription too much! But for the sake of this question, just pretend that no hack was necessary and $-3513.58$ was the actual value of $\log_2 p(u \mid \vec{w})$ as stated above.} Finally, $0.500 = \tfrac{4}{8}$ is the \textit{word error rate} of this transcription, which had 4 errors against the 8-word true transcription on the first line of the file; this will be used in question 9 below.\footnote{The word error rate of each transcription was computed for you by a scoring program, or ``scorer.'' The correct transcription on the first line sometimes contains special notation that the scorer paid attention to. For example, \texttt{*hesitation} on the first line told the scorer to count either \texttt{uh} or \texttt{um} as correct.}

We won’t actually make you write any code here. But according to Bayes’ Theorem, how should you choose among the 9 candidates? That is, what quantity are you trying to maximize, and how should you compute it?

(\textit{Hint:} You want to pick a candidate that both looks like English and looks like the audio utterance $u$. Your trigram model tells you about the former, and $-3513.58$ is an estimate of the latter.)

\section*{9 \textit{Extra credit}: Language modeling for speech recognition}

Actually implement the speech recognition selection method in question 8, using one of the language models you’ve already built. Use the \texttt{switchboard} corpus for training. You may experiment on the development set before getting your final results from the test set. When experimenting, you may want to start out with training on \texttt{switchboard-small}, just for speed.

\subsection*{(a)}
Modify \texttt{fileprob} to obtain a new program \texttt{speechrec} that chooses this best candidate. As usual, see \texttt{INSTRUCTIONS} for details.

The program should look at each utterance file listed on the command line, choose one of the 9 transcriptions according to Bayes’ Theorem, and report the word error rate of that transcription (as given in the first column). Finally, it should summarize the overall word error rate over all the utterances—the \textit{total} number of errors divided by the \textit{total} number of words in the correct transcriptions.

Of course, the program is not allowed to cheat: when choosing the transcription, it must ignore each file’s first row and first column!

Sample input (please allow this format):

\begin{verbatim}
./speechrec switchboard_whatever.model easy025 easy034
\end{verbatim}

Sample output (please use this format—but you are not required to get the same numbers):

\begin{verbatim}
0.125 easy025
0.037 easy034
0.057 OVERALL
\end{verbatim}

Notice that the overall error rate 0.057 is not an equal average of 0.125 and 0.037; this is because \texttt{easy034} is a longer utterance and counts more heavily.

\textit{Hints about how to read the file:}
\begin{itemize}
    \item For all lines but the first, you should read a few numbers, and then as many words as the integer told you to read (plus 2 for \texttt{<s>} and \texttt{</s>}). Alternatively, you could read the whole line at once and break it up into an array of whitespace-delimited strings.
    \item For the first line, you should read the initial integer, then read the rest of the line. The rest of the line is only there for your interest, so you can throw it away. The scorer has already considered the first line when computing the scores that start each remaining line. \textbf{Warning:} For the first line, the notational conventions are bizarre, so in this case the initial integer \textit{does not necessarily tell you} how many whitespace-delimited words are on the line. Thus, just throw away the rest of the line! (If necessary, read and discard characters up through the end-of-line symbol \texttt{\textbackslash n}.)
\end{itemize}

\subsection*{(b)}
What is your program’s overall error rate on the carefully chosen utterances in \texttt{test/easy}? How about on the random sample of utterances in \texttt{test/unrestricted}?

To get your answer, you need to choose a smoothing method, so pick one that seems to work well on the development data \texttt{dev/easy} and \texttt{dev/unrestricted}. Be sure to tell us which method you picked and why! What would be an \textit{unfair} way to choose a smoothing method?


\section*{10 \textit{Extra credit}: Open-vocabulary modeling}

We have been assuming a finite vocabulary by replacing all unknown words with a special OOV symbol. But an alternative is an open-vocabulary language model (reading section D.5).

Devise a sensible way to estimate the word trigram probability $p(z \mid xy)$ by backing off to a letter $n$-gram model of $z$ if $z$ is an unknown word. Also describe how you would train the letter $n$-gram model.

Just giving the formulas for your estimator will get you some extra credit. Implementing and testing them would be even better!

\textbf{Notes:}
\begin{itemize}
    \item $x$ and/or $y$ and/or $z$ may be unknown; be sure you make sensible estimates of $p(z \mid xy)$ in all these cases
    \item be sure that $\sum_z p(z \mid xy) = 1$
\end{itemize}


